{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11181182,"sourceType":"datasetVersion","datasetId":6979049}],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# A. Dataset and Preprocessing","metadata":{}},{"cell_type":"code","source":"import os\nimport pandas as pd\nimport re\nimport string\nimport nltk\n\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer\n\n# Define the text preprocessing function with stemming\ndef preprocess_text(text):\n    text = text.lower()\n    text = re.sub(r'[{}]'.format(re.escape(string.punctuation)), '', text)\n    text = text.encode(\"ascii\", \"ignore\").decode()  # Remove non-ASCII characters\n    tokens = word_tokenize(text)\n    stop_words = set(stopwords.words('english'))\n    tokens = [token for token in tokens if token not in stop_words]\n    \n    # Initialize the stemmer\n    stemmer = PorterStemmer()\n    tokens = [stemmer.stem(token) for token in tokens]  # Apply stemming to tokens\n    \n    processed_text = \" \".join(tokens)\n    return processed_text\n\n# Load the datasets (assuming the downloaded files are in the current directory)\ntrain_df = pd.read_csv('/kaggle/input/wiki-data/train.csv')\ntest_df = pd.read_csv('/kaggle/input/wiki-data/test.csv')\n\n# Create a validation set by randomly sampling 500 articles from the training set\nvalidation_df = train_df.sample(n=500, random_state=71)\n\n# Remove the selected validation articles from the training set\ntrain_df = train_df.drop(validation_df.index)\n\n# Preprocess the 'text' column for both training and validation sets\ntrain_df['processed_text'] = train_df['text'].apply(preprocess_text)\nvalidation_df['processed_text'] = validation_df['text'].apply(preprocess_text)\n\n# Saving the preprocessed datasets to new CSV files\ntrain_df.to_csv('train_preprocessed.csv', index=False)\nvalidation_df.to_csv('val_preprocessed.csv', index=False)\n\n# Print the shape of the datasets\nprint(\"Training set shape:\", train_df.shape)\nprint(\"Validation set shape:\", validation_df.shape)\n\n# Sample rows from the training set\nprint(\"Sample rows from the training set:\")\nprint(train_df.sample(2))\nprint(\"Sample rows from the validation set:\")\nprint(validation_df.sample(2))\n\nprint(\"Preprocessing complete. Training and validation sets are saved.\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-03-27T15:25:02.076106Z","iopub.execute_input":"2025-03-27T15:25:02.076446Z","iopub.status.idle":"2025-03-27T15:32:51.774394Z","shell.execute_reply.started":"2025-03-27T15:25:02.076418Z","shell.execute_reply":"2025-03-27T15:32:51.773593Z"}},"outputs":[{"name":"stdout","text":"Training set shape: (13379, 3)\nValidation set shape: (500, 3)\nSample rows from the training set:\n                   title                                               text  \\\n4314        Stefan Moore  Stefan Leroy Moore (born 28 September 1983) is...   \n6214  Columbus, Nebraska  Columbus is a city in and the county seat of P...   \n\n                                         processed_text  \n4314  stefan leroy moor born 28 septemb 1983 english...  \n6214  columbu citi counti seat platt counti state ne...  \nSample rows from the validation set:\n                title                                               text  \\\n5646   FC Dynamo Kyiv  Football Club Dynamo Kyiv (, ) is a Ukrainian ...   \n5676  Ottmar Hitzfeld  Ottmar Hitzfeld (; born 12 January 1949) is a ...   \n\n                                         processed_text  \n5646  footbal club dynamo kyiv ukrainian profession ...  \n5676  ottmar hitzfeld born 12 januari 1949 german fo...  \nPreprocessing complete. Training and validation sets are saved.\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"# B1. Setting up a basic RNN Seq2seq model","metadata":{}},{"cell_type":"markdown","source":"### Imports, Package Installation, and Seed Setup","metadata":{}},{"cell_type":"code","source":"# Imports and package installation\nimport re\nimport random\nimport math\nimport numpy as np\nfrom collections import Counter\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\n\n# Install ROUGE for evaluation\n!pip install rouge-score -q\nfrom rouge_score import rouge_scorer\n\n# Set random seeds for reproducibility\nrandom.seed(71)\nnp.random.seed(71)\ntorch.manual_seed(71)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T15:32:51.775598Z","iopub.execute_input":"2025-03-27T15:32:51.775876Z","iopub.status.idle":"2025-03-27T15:33:01.779442Z","shell.execute_reply.started":"2025-03-27T15:32:51.775854Z","shell.execute_reply":"2025-03-27T15:33:01.778374Z"}},"outputs":[{"name":"stdout","text":"  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n","output_type":"stream"},{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"<torch._C.Generator at 0x79537d108410>"},"metadata":{}}],"execution_count":2},{"cell_type":"markdown","source":"### Data Loading, Preprocessing, and Vocabulary Creation","metadata":{}},{"cell_type":"code","source":"# Load preprocessed training and validation data\n# (Assuming 'train_preprocessed.csv' has columns 'processed_text' for article bodies and 'title' for target titles.)\ntrain_df = pd.read_csv('train_preprocessed.csv')\nvalidation_df = pd.read_csv('val_preprocessed.csv')\n\n# Simple whitespace tokenizer function\ndef tokenize(text):\n    return text.strip().split()\n\n# Build vocabulary from training data based on tokens appearing in at least 1% of the documents.\nmin_occurrence = math.ceil(0.01 * len(train_df))\n\ndef build_vocab(texts, min_occurrence):\n    counter = Counter()\n    for text in texts:\n        tokens = set(tokenize(text))  # count once per document\n        counter.update(tokens)\n    vocab = {token for token, count in counter.items() if count >= min_occurrence}\n    return vocab\n\nsource_vocab = build_vocab(train_df['processed_text'], min_occurrence)\ntarget_vocab = build_vocab(train_df['title'], min_occurrence)\n\nprint(len(source_vocab))\nprint(len(target_vocab))\n\n# Create token-to-index mappings (reserve <pad>, <sos>, <eos>, <unk>)\ndef create_token2idx(vocab):\n    token2idx = {'<pad>': 0, '<sos>': 1, '<eos>': 2, '<unk>': 3}\n    for token in sorted(vocab):\n        token2idx[token] = len(token2idx)\n    return token2idx\n\nsrc_token2idx = create_token2idx(source_vocab)\ntgt_token2idx = create_token2idx(target_vocab)\n\n# For decoding, create an inverse mapping for target vocabulary\ntgt_idx2token = {idx: token for token, idx in tgt_token2idx.items()}\n\n# Define maximum sequence lengths for source and target\nSRC_MAX_LEN = 300   # article body max tokens\nTGT_MAX_LEN = 20    # title max tokens\n\n# Function to convert text to list of indices\ndef numericalize(text, token2idx, add_sos_eos=False, max_len=None):\n    tokens = tokenize(text)\n    if add_sos_eos:\n        tokens = ['<sos>'] + tokens + ['<eos>']\n    indices = [token2idx.get(token, token2idx['<unk>']) for token in tokens]\n    if max_len is not None:\n        indices = indices[:max_len]\n    return indices","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T15:33:01.781261Z","iopub.execute_input":"2025-03-27T15:33:01.781902Z","iopub.status.idle":"2025-03-27T15:33:10.250543Z","shell.execute_reply.started":"2025-03-27T15:33:01.781878Z","shell.execute_reply":"2025-03-27T15:33:10.249714Z"}},"outputs":[{"name":"stdout","text":"8081\n15\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"### Dataset and DataLoader Preparation","metadata":{}},{"cell_type":"code","source":"class WikiDataset(Dataset):\n    def __init__(self, df, src_token2idx, tgt_token2idx, src_max_len=SRC_MAX_LEN, tgt_max_len=TGT_MAX_LEN):\n        self.df = df\n        self.src_token2idx = src_token2idx\n        self.tgt_token2idx = tgt_token2idx\n        self.src_max_len = src_max_len\n        self.tgt_max_len = tgt_max_len\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        src_text = self.df.iloc[idx]['processed_text']\n        tgt_text = self.df.iloc[idx]['title']\n        src_indices = numericalize(src_text, self.src_token2idx, add_sos_eos=False, max_len=self.src_max_len)\n        tgt_indices = numericalize(tgt_text, self.tgt_token2idx, add_sos_eos=True, max_len=self.tgt_max_len)\n        return torch.tensor(src_indices), torch.tensor(tgt_indices)\n\ndef collate_fn(batch):\n    src_batch, tgt_batch = zip(*batch)\n    src_lens = [len(x) for x in src_batch]\n    tgt_lens = [len(x) for x in tgt_batch]\n    src_pad = nn.utils.rnn.pad_sequence(src_batch, batch_first=True, padding_value=src_token2idx['<pad>'])\n    tgt_pad = nn.utils.rnn.pad_sequence(tgt_batch, batch_first=True, padding_value=tgt_token2idx['<pad>'])\n    return src_pad, torch.tensor(src_lens), tgt_pad, torch.tensor(tgt_lens)\n\nBATCH_SIZE = 32\ntrain_dataset = WikiDataset(train_df, src_token2idx, tgt_token2idx)\nval_dataset = WikiDataset(validation_df, src_token2idx, tgt_token2idx)\n\ntrain_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\nval_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T15:33:10.251974Z","iopub.execute_input":"2025-03-27T15:33:10.252238Z","iopub.status.idle":"2025-03-27T15:33:10.259809Z","shell.execute_reply.started":"2025-03-27T15:33:10.252218Z","shell.execute_reply":"2025-03-27T15:33:10.258939Z"}},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"### Model Architecture – EncoderRNN, DecoderRNN, and Seq2seqRNN","metadata":{}},{"cell_type":"code","source":"HIDDEN_DIM = 300\nEMB_DIM = 300\n\n# 1. EncoderRNN class\nclass EncoderRNN(nn.Module):\n    def __init__(self, input_dim, emb_dim, hidden_dim, dropout=0.5):\n        super(EncoderRNN, self).__init__()\n        self.embedding = nn.Embedding(input_dim, emb_dim, padding_idx=src_token2idx['<pad>'])\n        self.gru = nn.GRU(emb_dim, hidden_dim, batch_first=True)\n        self.dropout = nn.Dropout(dropout)\n        \n    def forward(self, src, src_lengths):\n        # src: [batch_size, src_len]\n        embedded = self.dropout(self.embedding(src))\n        # Pack padded sequence\n        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, src_lengths.cpu(), batch_first=True, enforce_sorted=False)\n        packed_outputs, hidden = self.gru(packed_embedded)\n        outputs, _ = nn.utils.rnn.pad_packed_sequence(packed_outputs, batch_first=True)\n        # outputs: [batch_size, src_len, hidden_dim]\n        # hidden: [1, batch_size, hidden_dim]\n        return outputs, hidden\n\n# 2. DecoderRNN class\nclass DecoderRNN(nn.Module):\n    def __init__(self, output_dim, emb_dim, hidden_dim, dropout=0.5):\n        super(DecoderRNN, self).__init__()\n        self.embedding = nn.Embedding(output_dim, emb_dim, padding_idx=tgt_token2idx['<pad>'])\n        self.gru = nn.GRU(emb_dim, hidden_dim, batch_first=True)\n        self.fc_out = nn.Linear(hidden_dim, output_dim)\n        self.dropout = nn.Dropout(dropout)\n        self.relu = nn.ReLU()\n        \n    def forward(self, input, hidden):\n        # input: [batch_size] -> current token indices\n        input = input.unsqueeze(1)  # [batch_size, 1]\n        embedded = self.dropout(self.embedding(input))  # [batch_size, 1, emb_dim]\n        output, hidden = self.gru(embedded, hidden)  # output: [batch_size, 1, hidden_dim]\n        output = self.relu(output)\n        prediction = self.fc_out(output.squeeze(1))  # [batch_size, output_dim]\n        return prediction, hidden\n\n# 3. Seq2seqRNN class\nclass Seq2seqRNN(nn.Module):\n    def __init__(self, encoder, decoder, device):\n        super(Seq2seqRNN, self).__init__()\n        self.encoder = encoder\n        self.decoder = decoder\n        self.device = device\n        \n    def forward(self, src, src_lengths, tgt, teacher_forcing_ratio=0.5):\n        # src: [batch_size, src_len]\n        # tgt: [batch_size, tgt_len] with <sos> as first token\n        batch_size = src.size(0)\n        tgt_len = tgt.size(1)\n        tgt_vocab_size = self.decoder.embedding.num_embeddings\n        \n        # Tensor to store decoder outputs\n        outputs = torch.zeros(batch_size, tgt_len, tgt_vocab_size).to(self.device)\n        \n        encoder_outputs, hidden = self.encoder(src, src_lengths)\n        # First input token to decoder is <sos>\n        input = tgt[:, 0]  # [batch_size]\n        \n        for t in range(1, tgt_len):\n            output, hidden = self.decoder(input, hidden)\n            outputs[:, t] = output\n            teacher_force = random.random() < teacher_forcing_ratio\n            top1 = output.argmax(1)\n            input = tgt[:, t] if teacher_force else top1\n        return outputs\n\n# Instantiate the model\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nINPUT_DIM = len(src_token2idx)\nOUTPUT_DIM = len(tgt_token2idx)\n\nencoder = EncoderRNN(INPUT_DIM, EMB_DIM, HIDDEN_DIM).to(device)\ndecoder = DecoderRNN(OUTPUT_DIM, EMB_DIM, HIDDEN_DIM).to(device)\nmodel = Seq2seqRNN(encoder, decoder, device).to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T15:33:10.260811Z","iopub.execute_input":"2025-03-27T15:33:10.261085Z","iopub.status.idle":"2025-03-27T15:33:10.712283Z","shell.execute_reply.started":"2025-03-27T15:33:10.261053Z","shell.execute_reply":"2025-03-27T15:33:10.711401Z"}},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":"### Training and Evaluation Functions","metadata":{}},{"cell_type":"code","source":"optimizer = optim.Adam(model.parameters(), lr=0.001)\ncriterion = nn.CrossEntropyLoss(ignore_index=tgt_token2idx['<pad>'])\n\ndef train(model, loader, optimizer, criterion, clip=1):\n    model.train()\n    epoch_loss = 0\n    for src, src_lens, tgt, tgt_lens in loader:\n        src, tgt = src.to(device), tgt.to(device)\n        optimizer.zero_grad()\n        output = model(src, src_lens, tgt)  # output: [batch_size, tgt_len, output_dim]\n        output_dim = output.shape[-1]\n        # Exclude the first token (<sos>) for loss computation\n        output = output[:, 1:].reshape(-1, output_dim)\n        tgt = tgt[:, 1:].reshape(-1)\n        loss = criterion(output, tgt)\n        loss.backward()\n        nn.utils.clip_grad_norm_(model.parameters(), clip)\n        optimizer.step()\n        epoch_loss += loss.item()\n    return epoch_loss / len(loader)\n\ndef evaluate(model, loader, criterion):\n    model.eval()\n    epoch_loss = 0\n    with torch.no_grad():\n        for src, src_lens, tgt, tgt_lens in loader:\n            src, tgt = src.to(device), tgt.to(device)\n            output = model(src, src_lens, tgt, teacher_forcing_ratio=0)\n            output_dim = output.shape[-1]\n            output = output[:, 1:].reshape(-1, output_dim)\n            tgt = tgt[:, 1:].reshape(-1)\n            loss = criterion(output, tgt)\n            epoch_loss += loss.item()\n    return epoch_loss / len(loader)\n\nN_EPOCHS = 10\nCLIP = 1\n\nbest_valid_loss = float('inf')\nfor epoch in range(N_EPOCHS):\n    train_loss = train(model, train_loader, optimizer, criterion, clip=CLIP)\n    valid_loss = evaluate(model, val_loader, criterion)\n    if valid_loss < best_valid_loss:\n        best_valid_loss = valid_loss\n        torch.save(model.state_dict(), 'seq2seq_model.pt')\n    print(f'Epoch: {epoch+1:02} | Train Loss: {train_loss:.3f} | Val Loss: {valid_loss:.3f}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T15:33:10.713359Z","iopub.execute_input":"2025-03-27T15:33:10.713666Z","iopub.status.idle":"2025-03-27T15:36:15.991162Z","shell.execute_reply.started":"2025-03-27T15:33:10.713643Z","shell.execute_reply":"2025-03-27T15:36:15.990365Z"}},"outputs":[{"name":"stdout","text":"Epoch: 01 | Train Loss: 0.698 | Val Loss: 0.706\nEpoch: 02 | Train Loss: 0.531 | Val Loss: 0.534\nEpoch: 03 | Train Loss: 0.404 | Val Loss: 0.410\nEpoch: 04 | Train Loss: 0.346 | Val Loss: 0.406\nEpoch: 05 | Train Loss: 0.312 | Val Loss: 0.394\nEpoch: 06 | Train Loss: 0.289 | Val Loss: 0.367\nEpoch: 07 | Train Loss: 0.267 | Val Loss: 0.372\nEpoch: 08 | Train Loss: 0.249 | Val Loss: 0.364\nEpoch: 09 | Train Loss: 0.236 | Val Loss: 0.368\nEpoch: 10 | Train Loss: 0.227 | Val Loss: 0.355\n","output_type":"stream"}],"execution_count":6},{"cell_type":"markdown","source":"### Inference and ROUGE Evaluation","metadata":{}},{"cell_type":"code","source":"def generate_title(model, src_sequence, src_length, max_len=TGT_MAX_LEN):\n    model.eval()\n    src_sequence = src_sequence.unsqueeze(0).to(device)  # [1, src_len]\n    src_length = torch.tensor([src_length]).to(device)\n    with torch.no_grad():\n        encoder_outputs, hidden = model.encoder(src_sequence, src_length)\n    # Start with <sos> token\n    input_token = torch.tensor([tgt_token2idx['<sos>']]).to(device)\n    generated_tokens = []\n    for _ in range(max_len):\n        with torch.no_grad():\n            output, hidden = model.decoder(input_token, hidden)\n            top1 = output.argmax(1)\n        if top1.item() == tgt_token2idx['<eos>']:\n            break\n        generated_tokens.append(top1.item())\n        input_token = top1\n    title = \" \".join([tgt_idx2token.get(idx, '<unk>') for idx in generated_tokens])\n    return title\n\n# Load test set (assuming a 'text' column exists)\ntest_df = pd.read_csv('/kaggle/input/wiki-data/test.csv')\n# If not preprocessed, apply a simple preprocessing (e.g., lowercasing and punctuation removal)\nif 'processed_text' not in test_df.columns:\n    test_df['processed_text'] = test_df['text'].apply(lambda x: re.sub(r'[^\\w\\s]', ' ', x.lower().encode('ascii', errors='ignore').decode()))\n\npredictions = []\nreferences = []  # Populate if test set includes reference titles\n\nfor idx, row in test_df.iterrows():\n    src_indices = numericalize(row['processed_text'], src_token2idx, add_sos_eos=False, max_len=SRC_MAX_LEN)\n    title_pred = generate_title(model, torch.tensor(src_indices), len(src_indices))\n    predictions.append(title_pred)\n    # If test set has reference titles in a 'title' column\n    if 'title' in test_df.columns:\n        references.append(row['title'])\n    else:\n        references.append(\"\")\n\n# Compute ROUGE scores if reference titles exist\nif any(references):\n    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n    rouge1_scores, rouge2_scores, rougeL_scores = [], [], []\n    for ref, pred in zip(references, predictions):\n        scores = scorer.score(ref, pred)\n        rouge1_scores.append(scores['rouge1'].fmeasure)\n        rouge2_scores.append(scores['rouge2'].fmeasure)\n        rougeL_scores.append(scores['rougeL'].fmeasure)\n    avg_rouge1 = np.mean(rouge1_scores)\n    avg_rouge2 = np.mean(rouge2_scores)\n    avg_rougeL = np.mean(rougeL_scores)\n    print(f\"ROUGE-1 F1: {avg_rouge1:.4f}\")\n    print(f\"ROUGE-2 F1: {avg_rouge2:.4f}\")\n    print(f\"ROUGE-L F1: {avg_rougeL:.4f}\")\nelse:\n    print(\"No reference titles available for ROUGE evaluation.\")\n\nprint(\"Title generation complete.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T15:36:15.992194Z","iopub.execute_input":"2025-03-27T15:36:15.992813Z","iopub.status.idle":"2025-03-27T15:36:16.751086Z","shell.execute_reply.started":"2025-03-27T15:36:15.992775Z","shell.execute_reply":"2025-03-27T15:36:16.750358Z"}},"outputs":[{"name":"stdout","text":"ROUGE-1 F1: 0.0327\nROUGE-2 F1: 0.0000\nROUGE-L F1: 0.0327\nTitle generation complete.\n","output_type":"stream"}],"execution_count":7},{"cell_type":"markdown","source":"# B2. Improving the RNN model","metadata":{}},{"cell_type":"markdown","source":"### Imports","metadata":{}},{"cell_type":"code","source":"import os\nimport re\nimport random\nimport math\nimport numpy as np\nfrom collections import Counter\n\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\n\n# Install ROUGE for evaluation\n!pip install rouge-score -q\nfrom rouge_score import rouge_scorer\n\n# Set seeds for reproducibility\nrandom.seed(71)\nnp.random.seed(71)\ntorch.manual_seed(71)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T15:36:16.751795Z","iopub.execute_input":"2025-03-27T15:36:16.752089Z","iopub.status.idle":"2025-03-27T15:36:19.993503Z","shell.execute_reply.started":"2025-03-27T15:36:16.752067Z","shell.execute_reply":"2025-03-27T15:36:19.992312Z"}},"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"<torch._C.Generator at 0x79537d108410>"},"metadata":{}}],"execution_count":8},{"cell_type":"markdown","source":"### Data Loading, Preprocessing, and Vocabulary Creation","metadata":{}},{"cell_type":"code","source":"# Assume train_preprocessed.csv and val_preprocessed.csv exist with columns 'processed_text' and 'title'\ntrain_df = pd.read_csv('train_preprocessed.csv')\nvalidation_df = pd.read_csv('val_preprocessed.csv')\n\ndef tokenize(text):\n    return text.strip().split()\n\nmin_occurrence = math.ceil(0.01 * len(train_df))\ndef build_vocab(texts, min_occurrence):\n    counter = Counter()\n    for text in texts:\n        tokens = set(tokenize(text))\n        counter.update(tokens)\n    vocab = {token for token, count in counter.items() if count >= min_occurrence}\n    return vocab\n\nsource_vocab = build_vocab(train_df['processed_text'], min_occurrence)\ntarget_vocab = build_vocab(train_df['title'], min_occurrence)\n\ndef create_token2idx(vocab):\n    token2idx = {'<pad>': 0, '<sos>': 1, '<eos>': 2, '<unk>': 3}\n    for token in sorted(vocab):\n        token2idx[token] = len(token2idx)\n    return token2idx\n\nsrc_token2idx = create_token2idx(source_vocab)\ntgt_token2idx = create_token2idx(target_vocab)\ntgt_idx2token = {idx: token for token, idx in tgt_token2idx.items()}\n\nSRC_MAX_LEN = 300  # maximum tokens for article body\nTGT_MAX_LEN = 20   # maximum tokens for title\n\ndef numericalize(text, token2idx, add_sos_eos=False, max_len=None):\n    tokens = tokenize(text)\n    if add_sos_eos:\n        tokens = ['<sos>'] + tokens + ['<eos>']\n    indices = [token2idx.get(token, token2idx['<unk>']) for token in tokens]\n    if max_len is not None:\n        indices = indices[:max_len]\n    return indices","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T15:36:19.996897Z","iopub.execute_input":"2025-03-27T15:36:19.997171Z","iopub.status.idle":"2025-03-27T15:36:28.155985Z","shell.execute_reply.started":"2025-03-27T15:36:19.997149Z","shell.execute_reply":"2025-03-27T15:36:28.155313Z"}},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":"### Dataset and DataLoader Preparation","metadata":{}},{"cell_type":"code","source":"class WikiDataset(Dataset):\n    def __init__(self, df, src_token2idx, tgt_token2idx, src_max_len=SRC_MAX_LEN, tgt_max_len=TGT_MAX_LEN):\n        self.df = df\n        self.src_token2idx = src_token2idx\n        self.tgt_token2idx = tgt_token2idx\n        self.src_max_len = src_max_len\n        self.tgt_max_len = tgt_max_len\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        src_text = self.df.iloc[idx]['processed_text']\n        tgt_text = self.df.iloc[idx]['title']\n        src_indices = numericalize(src_text, self.src_token2idx, add_sos_eos=False, max_len=self.src_max_len)\n        tgt_indices = numericalize(tgt_text, self.tgt_token2idx, add_sos_eos=True, max_len=self.tgt_max_len)\n        return torch.tensor(src_indices), torch.tensor(tgt_indices)\n\ndef collate_fn(batch):\n    src_batch, tgt_batch = zip(*batch)\n    src_lens = [len(x) for x in src_batch]\n    tgt_lens = [len(x) for x in tgt_batch]\n    src_pad = nn.utils.rnn.pad_sequence(src_batch, batch_first=True, padding_value=src_token2idx['<pad>'])\n    tgt_pad = nn.utils.rnn.pad_sequence(tgt_batch, batch_first=True, padding_value=tgt_token2idx['<pad>'])\n    return src_pad, torch.tensor(src_lens), tgt_pad, torch.tensor(tgt_lens)\n\nBATCH_SIZE = 32\ntrain_dataset = WikiDataset(train_df, src_token2idx, tgt_token2idx)\nval_dataset = WikiDataset(validation_df, src_token2idx, tgt_token2idx)\n\ntrain_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\nval_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T15:36:28.157944Z","iopub.execute_input":"2025-03-27T15:36:28.158270Z","iopub.status.idle":"2025-03-27T15:36:28.208707Z","shell.execute_reply.started":"2025-03-27T15:36:28.158247Z","shell.execute_reply":"2025-03-27T15:36:28.207819Z"}},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":"### Model Architecture","metadata":{}},{"cell_type":"markdown","source":"#### EncoderRNN with GloVe Loader","metadata":{}},{"cell_type":"code","source":"HIDDEN_DIM = 300\nEMB_DIM = 300\n\nclass EncoderRNN(nn.Module):\n    def __init__(self, input_dim, emb_dim, hidden_dim, dropout=0.5):\n        super(EncoderRNN, self).__init__()\n        self.embedding = nn.Embedding(input_dim, emb_dim, padding_idx=src_token2idx['<pad>'])\n        self.gru = nn.GRU(emb_dim, hidden_dim, batch_first=True)\n        self.dropout = nn.Dropout(dropout)\n    \n    def load_embeddings(self, glove_path, token2idx):\n        \"\"\"\n        glove_path: path to GloVe file (e.g., 'glove.6B.300d.txt')\n        token2idx: vocabulary mapping used in this encoder.\n        \"\"\"\n        print(\"Loading GloVe embeddings...\")\n        # Create a random embedding matrix\n        embedding_matrix = np.random.normal(size=(len(token2idx), EMB_DIM))\n        # Read the GloVe file and update the matrix for known tokens\n        with open(glove_path, 'r', encoding='utf-8') as f:\n            for line in f:\n                parts = line.strip().split()\n                word = parts[0]\n                if word in token2idx:\n                    vector = np.array(parts[1:], dtype='float32')\n                    embedding_matrix[token2idx[word]] = vector\n        # Set the embedding weights and freeze them if desired\n        self.embedding.weight.data.copy_(torch.from_numpy(embedding_matrix))\n        print(\"GloVe embeddings loaded.\")\n    \n    def forward(self, src, src_lengths):\n        embedded = self.dropout(self.embedding(src))\n        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, src_lengths.cpu(), batch_first=True, enforce_sorted=False)\n        packed_outputs, hidden = self.gru(packed_embedded)\n        outputs, _ = nn.utils.rnn.pad_packed_sequence(packed_outputs, batch_first=True)\n        return outputs, hidden\n\n# Define the expected path for the GloVe file.\nglove_path = '/kaggle/working/glove.6B.300d.txt'\n\n# If the GloVe file does not exist, download and unzip it.\nif not os.path.exists(glove_path):\n    print(\"GloVe file not found. Downloading and extracting...\")\n    # Download the GloVe zip file from the Stanford NLP website.\n    !wget http://nlp.stanford.edu/data/glove.6B.zip -O /kaggle/working/glove.6B.zip\n    # Unzip the downloaded file into the working directory.\n    !unzip /kaggle/working/glove.6B.zip -d /kaggle/working/\n    print(\"GloVe file downloaded and extracted.\")\nelse:\n    print(\"GloVe file found.\")\n\n# Instantiate the encoder (input_dim equals the size of src_token2idx)\nencoder = EncoderRNN(input_dim=len(src_token2idx), emb_dim=EMB_DIM, hidden_dim=HIDDEN_DIM)\n\n# Load GloVe embeddings into the encoder.\nencoder.load_embeddings(glove_path, src_token2idx)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T15:36:28.210581Z","iopub.execute_input":"2025-03-27T15:36:28.210802Z","iopub.status.idle":"2025-03-27T16:25:17.015769Z","shell.execute_reply.started":"2025-03-27T15:36:28.210785Z","shell.execute_reply":"2025-03-27T16:25:17.014677Z"}},"outputs":[{"name":"stdout","text":"GloVe file not found. Downloading and extracting...\n--2025-03-27 15:36:28--  http://nlp.stanford.edu/data/glove.6B.zip\nResolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\nConnecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\nHTTP request sent, awaiting response... 302 Found\nLocation: https://nlp.stanford.edu/data/glove.6B.zip [following]\n--2025-03-27 15:36:28--  https://nlp.stanford.edu/data/glove.6B.zip\nConnecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\nHTTP request sent, awaiting response... 301 Moved Permanently\nLocation: https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n--2025-03-27 15:36:29--  https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\nResolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\nConnecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 862182613 (822M) [application/zip]\nSaving to: ‘/kaggle/working/glove.6B.zip’\n\n/kaggle/working/glo 100%[===================>] 822.24M   210KB/s    in 48m 9s  \n\n2025-03-27 16:24:49 (291 KB/s) - ‘/kaggle/working/glove.6B.zip’ saved [862182613/862182613]\n\nArchive:  /kaggle/working/glove.6B.zip\n  inflating: /kaggle/working/glove.6B.50d.txt  \n  inflating: /kaggle/working/glove.6B.100d.txt  \n  inflating: /kaggle/working/glove.6B.200d.txt  \n  inflating: /kaggle/working/glove.6B.300d.txt  \nGloVe file downloaded and extracted.\nLoading GloVe embeddings...\nGloVe embeddings loaded.\n","output_type":"stream"}],"execution_count":11},{"cell_type":"markdown","source":"#### HierEncoderRNN","metadata":{}},{"cell_type":"code","source":"class HierEncoderRNN(nn.Module):\n    def __init__(self, input_dim, emb_dim, hidden_dim, dropout=0.5):\n        super(HierEncoderRNN, self).__init__()\n        self.embedding = nn.Embedding(input_dim, emb_dim, padding_idx=src_token2idx['<pad>'])\n        self.sentence_gru = nn.GRU(emb_dim, hidden_dim, batch_first=True)\n        self.document_gru = nn.GRU(hidden_dim, hidden_dim, batch_first=True)\n        self.dropout = nn.Dropout(dropout)\n    \n    def forward(self, src, src_lengths):\n        # Here, we assume each article is a string of tokens that we split into sentences by period.\n        # In practice, you should pre-segment sentences.\n        batch_size = src.size(0)\n        outputs_all = []\n        hidden_sentences = []\n        for i in range(batch_size):\n            # Convert indices back to tokens (simple join, then split by period)\n            tokens = [tgt_idx2token.get(idx.item(), '<unk>') for idx in src[i]]\n            text = \" \".join(tokens)\n            sentences = text.split('.')\n            sentence_embeddings = []\n            for sent in sentences:\n                sent_tokens = tokenize(sent.strip())\n                if not sent_tokens:\n                    continue\n                # Convert sentence tokens to indices (ignoring OOV for simplicity)\n                indices = [src_token2idx.get(token, src_token2idx['<unk>']) for token in sent_tokens]\n                sent_tensor = torch.tensor(indices).unsqueeze(0).to(src.device)\n                sent_emb = self.dropout(self.embedding(sent_tensor))\n                _, sent_hidden = self.sentence_gru(sent_emb)\n                sentence_embeddings.append(sent_hidden.squeeze(0))\n            if len(sentence_embeddings) == 0:\n                sentence_embeddings.append(torch.zeros(self.sentence_gru.hidden_size).to(src.device))\n            sentence_embeddings = torch.stack(sentence_embeddings).unsqueeze(0)  # [1, num_sent, hidden_dim]\n            _, doc_hidden = self.document_gru(sentence_embeddings)\n            outputs_all.append(sentence_embeddings)\n            hidden_sentences.append(doc_hidden)\n        # Stack hidden states\n        hidden = torch.stack(hidden_sentences, dim=1)  # [1, batch, hidden_dim]\n        # Note: outputs are not used in our basic decoder; only final hidden state is returned.\n        return None, hidden","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T16:25:17.016836Z","iopub.execute_input":"2025-03-27T16:25:17.017159Z","iopub.status.idle":"2025-03-27T16:25:17.028003Z","shell.execute_reply.started":"2025-03-27T16:25:17.017085Z","shell.execute_reply":"2025-03-27T16:25:17.027222Z"}},"outputs":[],"execution_count":12},{"cell_type":"markdown","source":"#### DecoderRNN and Decoder2RNN","metadata":{}},{"cell_type":"code","source":"# Standard decoder (already defined before)\nclass DecoderRNN(nn.Module):\n    def __init__(self, output_dim, emb_dim, hidden_dim, dropout=0.5):\n        super(DecoderRNN, self).__init__()\n        self.embedding = nn.Embedding(output_dim, emb_dim, padding_idx=tgt_token2idx['<pad>'])\n        self.gru = nn.GRU(emb_dim, hidden_dim, batch_first=True)\n        self.fc_out = nn.Linear(hidden_dim, output_dim)\n        self.dropout = nn.Dropout(dropout)\n        self.relu = nn.ReLU()\n        \n    def forward(self, input, hidden):\n        input = input.unsqueeze(1)\n        embedded = self.dropout(self.embedding(input))\n        output, hidden = self.gru(embedded, hidden)\n        output = self.relu(output)\n        prediction = self.fc_out(output.squeeze(1))\n        return prediction, hidden\n\n# Alternative decoder with two GRU layers\nclass Decoder2RNN(nn.Module):\n    def __init__(self, output_dim, emb_dim, hidden_dim, dropout=0.5):\n        super(Decoder2RNN, self).__init__()\n        self.embedding = nn.Embedding(output_dim, emb_dim, padding_idx=tgt_token2idx['<pad>'])\n        self.gru1 = nn.GRU(emb_dim, hidden_dim, batch_first=True)\n        self.gru2 = nn.GRU(hidden_dim, hidden_dim, batch_first=True)\n        self.fc_out = nn.Linear(hidden_dim, output_dim)\n        self.dropout = nn.Dropout(dropout)\n        self.relu = nn.ReLU()\n        \n    def forward(self, input, hidden):\n        input = input.unsqueeze(1)\n        embedded = self.dropout(self.embedding(input))\n        output1, hidden1 = self.gru1(embedded, hidden)\n        output2, hidden2 = self.gru2(output1, hidden1)\n        output2 = self.relu(output2)\n        prediction = self.fc_out(output2.squeeze(1))\n        return prediction, hidden2","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T16:25:17.028928Z","iopub.execute_input":"2025-03-27T16:25:17.029241Z","iopub.status.idle":"2025-03-27T16:25:17.048373Z","shell.execute_reply.started":"2025-03-27T16:25:17.029211Z","shell.execute_reply":"2025-03-27T16:25:17.047733Z"}},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":"#### Seq2seqRNN with Encoder/Decoder Choice and Beam Search","metadata":{}},{"cell_type":"code","source":"class Seq2seqRNN(nn.Module):\n    def __init__(self, encoder, decoder, device, use_beam_search=False):\n        \"\"\"\n        encoder: an instance of EncoderRNN or HierEncoderRNN\n        decoder: an instance of DecoderRNN or Decoder2RNN\n        use_beam_search: default decoding mode is greedy if False.\n        \"\"\"\n        super(Seq2seqRNN, self).__init__()\n        self.encoder = encoder\n        self.decoder = decoder\n        self.device = device\n        self.use_beam_search = use_beam_search\n\n    def forward(self, src, src_lengths, tgt, teacher_forcing_ratio=0.5, beam_width=3):\n        # If not using beam search, perform standard (greedy) decoding during training.\n        if not self.use_beam_search:\n            batch_size = src.size(0)\n            tgt_len = tgt.size(1)\n            tgt_vocab_size = self.decoder.embedding.num_embeddings\n            outputs = torch.zeros(batch_size, tgt_len, tgt_vocab_size).to(self.device)\n            encoder_outputs, hidden = self.encoder(src, src_lengths)\n            input = tgt[:, 0]  # <sos>\n            for t in range(1, tgt_len):\n                output, hidden = self.decoder(input, hidden)\n                outputs[:, t] = output\n                teacher_force = random.random() < teacher_forcing_ratio\n                top1 = output.argmax(1)\n                input = tgt[:, t] if teacher_force else top1\n            return outputs\n        else:\n            # Beam search decoding (applied at inference only)\n            return self.beam_search_decode(src, src_lengths, beam_width=beam_width)\n\n    def beam_search_decode(self, src, src_lengths, beam_width=3, max_len=TGT_MAX_LEN):\n        self.encoder.eval()\n        self.decoder.eval()\n        batch_size = src.size(0)\n        # We assume batch_size=1 for beam search decoding\n        assert batch_size == 1, \"Beam search decoding is implemented for batch_size=1.\"\n        encoder_outputs, hidden = self.encoder(src, src_lengths)\n        # Start with <sos>\n        init_token = tgt_token2idx['<sos>']\n        beams = [([init_token], hidden, 0)]  # (sequence, hidden state, cumulative log prob)\n\n        for _ in range(max_len):\n            new_beams = []\n            for seq, hidden_state, score in beams:\n                last_token = torch.tensor([seq[-1]]).to(self.device)\n                # Stop if <eos> already generated\n                if seq[-1] == tgt_token2idx['<eos>']:\n                    new_beams.append((seq, hidden_state, score))\n                    continue\n                output, hidden_new = self.decoder(last_token, hidden_state)\n                log_probs = torch.log_softmax(output, dim=1)\n                top_log_probs, top_indices = log_probs.topk(beam_width)\n                for log_prob, idx in zip(top_log_probs[0], top_indices[0]):\n                    new_seq = seq + [idx.item()]\n                    new_score = score + log_prob.item()\n                    new_beams.append((new_seq, hidden_new, new_score))\n            # Keep top beams\n            beams = sorted(new_beams, key=lambda x: x[2], reverse=True)[:beam_width]\n            # If all beams ended with <eos>, break early\n            if all(seq[-1] == tgt_token2idx['<eos>'] for seq, _, _ in beams):\n                break\n        # Return the highest scoring sequence (excluding the <sos>)\n        best_seq = beams[0][0][1:]\n        return best_seq  # list of token indices","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T16:25:17.049193Z","iopub.execute_input":"2025-03-27T16:25:17.049417Z","iopub.status.idle":"2025-03-27T16:25:17.069443Z","shell.execute_reply.started":"2025-03-27T16:25:17.049390Z","shell.execute_reply":"2025-03-27T16:25:17.068242Z"}},"outputs":[],"execution_count":14},{"cell_type":"markdown","source":"### Training and Evaluation Functions","metadata":{}},{"cell_type":"code","source":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\noptimizer = optim.Adam(model.parameters(), lr=0.001)\ncriterion = nn.CrossEntropyLoss(ignore_index=tgt_token2idx['<pad>'])\n\ndef train(model, loader, optimizer, criterion, clip=1):\n    model.train()\n    epoch_loss = 0\n    for src, src_lens, tgt, tgt_lens in loader:\n        src, tgt = src.to(device), tgt.to(device)\n        optimizer.zero_grad()\n        output = model(src, src_lens, tgt, teacher_forcing_ratio=0.5)\n        output_dim = output.shape[-1]\n        output = output[:, 1:].reshape(-1, output_dim)\n        tgt = tgt[:, 1:].reshape(-1)\n        loss = criterion(output, tgt)\n        loss.backward()\n        nn.utils.clip_grad_norm_(model.parameters(), clip)\n        optimizer.step()\n        epoch_loss += loss.item()\n    return epoch_loss / len(loader)\n\ndef evaluate(model, loader, criterion):\n    model.eval()\n    epoch_loss = 0\n    with torch.no_grad():\n        for src, src_lens, tgt, tgt_lens in loader:\n            src, tgt = src.to(device), tgt.to(device)\n            output = model(src, src_lens, tgt, teacher_forcing_ratio=0)\n            output_dim = output.shape[-1]\n            output = output[:, 1:].reshape(-1, output_dim)\n            tgt = tgt[:, 1:].reshape(-1)\n            loss = criterion(output, tgt)\n            epoch_loss += loss.item()\n    return epoch_loss / len(loader)\n\nN_EPOCHS = 10\nCLIP = 1\n\nbest_valid_loss = float('inf')\nfor epoch in range(N_EPOCHS):\n    train_loss = train(model, train_loader, optimizer, criterion, clip=CLIP)\n    valid_loss = evaluate(model, val_loader, criterion)\n    if valid_loss < best_valid_loss:\n        best_valid_loss = valid_loss\n        torch.save(model.state_dict(), 'seq2seq_model.pt')\n    print(f'Epoch: {epoch+1:02} | Train Loss: {train_loss:.3f} | Val Loss: {valid_loss:.3f}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T16:25:17.070376Z","iopub.execute_input":"2025-03-27T16:25:17.070661Z","iopub.status.idle":"2025-03-27T16:28:30.635883Z","shell.execute_reply.started":"2025-03-27T16:25:17.070629Z","shell.execute_reply":"2025-03-27T16:28:30.634913Z"}},"outputs":[{"name":"stdout","text":"Epoch: 01 | Train Loss: 0.218 | Val Loss: 0.394\nEpoch: 02 | Train Loss: 0.205 | Val Loss: 0.369\nEpoch: 03 | Train Loss: 0.199 | Val Loss: 0.379\nEpoch: 04 | Train Loss: 0.190 | Val Loss: 0.409\nEpoch: 05 | Train Loss: 0.185 | Val Loss: 0.411\nEpoch: 06 | Train Loss: 0.180 | Val Loss: 0.432\nEpoch: 07 | Train Loss: 0.176 | Val Loss: 0.396\nEpoch: 08 | Train Loss: 0.170 | Val Loss: 0.411\nEpoch: 09 | Train Loss: 0.169 | Val Loss: 0.422\nEpoch: 10 | Train Loss: 0.162 | Val Loss: 0.411\n","output_type":"stream"}],"execution_count":15},{"cell_type":"markdown","source":"### Inference and ROUGE Evaluation","metadata":{}},{"cell_type":"code","source":"def generate_title(model, src_sequence, src_length, use_beam_search_flag=False, beam_width=3):\n    model.eval()\n    src_sequence = src_sequence.unsqueeze(0).to(device)\n    src_length = torch.tensor([src_length]).to(device)\n    \n    if not use_beam_search_flag:\n        with torch.no_grad():\n            encoder_outputs, hidden = model.encoder(src_sequence, src_length)\n        input_token = torch.tensor([tgt_token2idx['<sos>']]).to(device)\n        generated_tokens = []\n        for _ in range(TGT_MAX_LEN):\n            with torch.no_grad():\n                output, hidden = model.decoder(input_token, hidden)\n                top1 = output.argmax(1)\n            if top1.item() == tgt_token2idx['<eos>']:\n                break\n            generated_tokens.append(top1.item())\n            input_token = top1\n    else:\n        with torch.no_grad():\n            best_seq = model.beam_search_decode(src_sequence, src_length, beam_width=beam_width, max_len=TGT_MAX_LEN)\n        generated_tokens = best_seq\n    \n    title = \" \".join([tgt_idx2token.get(idx, '<unk>') for idx in generated_tokens])\n    return title\n\n# Load test set (assumes a 'text' column exists)\ntest_df = pd.read_csv('/kaggle/input/wiki-data/test.csv')\nif 'processed_text' not in test_df.columns:\n    test_df['processed_text'] = test_df['text'].apply(lambda x: re.sub(r'[^\\w\\s]', ' ', x.lower().encode('ascii', errors='ignore').decode()))\n\npredictions = []\nreferences = []  # populate if test set has reference titles\n\nfor idx, row in test_df.iterrows():\n    src_indices = numericalize(row['processed_text'], src_token2idx, add_sos_eos=False, max_len=SRC_MAX_LEN)\n    title_pred = generate_title(model, torch.tensor(src_indices), len(src_indices), use_beam_search_flag=False, beam_width=3)\n    predictions.append(title_pred)\n    if 'title' in test_df.columns:\n        references.append(row['title'])\n    else:\n        references.append(\"\")\n\nif any(references):\n    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n    rouge1_scores, rouge2_scores, rougeL_scores = [], [], []\n    for ref, pred in zip(references, predictions):\n        scores = scorer.score(ref, pred)\n        rouge1_scores.append(scores['rouge1'].fmeasure)\n        rouge2_scores.append(scores['rouge2'].fmeasure)\n        rougeL_scores.append(scores['rougeL'].fmeasure)\n    print(f\"ROUGE-1 F1: {np.mean(rouge1_scores):.4f}\")\n    print(f\"ROUGE-2 F1: {np.mean(rouge2_scores):.4f}\")\n    print(f\"ROUGE-L F1: {np.mean(rougeL_scores):.4f}\")\nelse:\n    print(\"No reference titles available for ROUGE evaluation.\")\n\nprint(\"Title generation complete.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T16:36:18.993870Z","iopub.execute_input":"2025-03-27T16:36:18.994194Z","iopub.status.idle":"2025-03-27T16:36:19.761433Z","shell.execute_reply.started":"2025-03-27T16:36:18.994172Z","shell.execute_reply":"2025-03-27T16:36:19.760594Z"}},"outputs":[{"name":"stdout","text":"ROUGE-1 F1: 0.0397\nROUGE-2 F1: 0.0033\nROUGE-L F1: 0.0397\nTitle generation complete.\n","output_type":"stream"}],"execution_count":19},{"cell_type":"markdown","source":"# C1. Attention is all you need!","metadata":{}},{"cell_type":"markdown","source":"### Imports and Environment Setup","metadata":{}},{"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\n!pip install evaluate\nimport evaluate\nimport nltk\nfrom datasets import Dataset, DatasetDict\nfrom transformers import (AutoTokenizer, AutoModelForSeq2SeqLM, \n                          Seq2SeqTrainer, Seq2SeqTrainingArguments, DataCollatorForSeq2Seq)\nimport torch\n\n# Set random seeds for reproducibility\nnp.random.seed(71)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T16:37:25.686168Z","iopub.execute_input":"2025-03-27T16:37:25.686451Z","iopub.status.idle":"2025-03-27T16:37:46.034454Z","shell.execute_reply.started":"2025-03-27T16:37:25.686430Z","shell.execute_reply":"2025-03-27T16:37:46.033469Z"}},"outputs":[{"name":"stdout","text":"Collecting evaluate\n  Downloading evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\nRequirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (3.3.1)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from evaluate) (1.26.4)\nRequirement already satisfied: dill in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.3.8)\nRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.2.3)\nRequirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.32.3)\nRequirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from evaluate) (4.67.1)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from evaluate) (3.5.0)\nRequirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.70.16)\nRequirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2024.12.0)\nRequirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.29.0)\nRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from evaluate) (24.2)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (3.17.0)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (19.0.1)\nRequirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (3.11.12)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (6.0.2)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.12.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->evaluate) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->evaluate) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->evaluate) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->evaluate) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->evaluate) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->evaluate) (2.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2025.1.31)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2025.1)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2025.1)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (2.4.6)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.2)\nRequirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (5.0.1)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (25.1.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.5.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.1.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (0.2.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.18.3)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.17.0)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->evaluate) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->evaluate) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->evaluate) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.17->evaluate) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.17->evaluate) (2024.2.0)\nDownloading evaluate-0.4.3-py3-none-any.whl (84 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: evaluate\nSuccessfully installed evaluate-0.4.3\n","output_type":"stream"}],"execution_count":21},{"cell_type":"markdown","source":"### Load Pretrained T5 Model","metadata":{}},{"cell_type":"code","source":"model_name = \"google-t5/t5-small\"  # Pretrained T5-small\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForSeq2SeqLM.from_pretrained(model_name)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T16:37:46.035859Z","iopub.execute_input":"2025-03-27T16:37:46.036471Z","iopub.status.idle":"2025-03-27T16:37:58.484535Z","shell.execute_reply.started":"2025-03-27T16:37:46.036447Z","shell.execute_reply":"2025-03-27T16:37:58.483483Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/2.32k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"40a4a047dce04d6e881a155abecd2727"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ed0c9b5336c542b5b3723c1623dad7fa"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.39M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c9f651c1ce964f53baa42cb62cfe15d7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.21k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dc9d386a0c8940a39d61f7207664fdcb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/242M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3c8885670e2e4f448a3a0cfe5d32da28"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"36621e07aa314209ac0153cd55047f52"}},"metadata":{}}],"execution_count":22},{"cell_type":"markdown","source":"### Load Raw Dataset","metadata":{}},{"cell_type":"code","source":"# Load the training data\ntrain_df = pd.read_csv('/kaggle/input/wiki-data/train.csv')\n\n# Extract a validation set (e.g., 10% of the training data)\nvalidation_ratio = 0.1\nval_df = train_df.sample(frac=validation_ratio, random_state=42)\ntrain_df = train_df.drop(val_df.index).reset_index(drop=True)\nval_df = val_df.reset_index(drop=True)\n\nprint(\"Training set size:\", len(train_df))\nprint(\"Validation set size:\", len(validation_df))\n\n\ntest_df = pd.read_csv('/kaggle/input/wiki-data/test.csv')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T16:37:58.486258Z","iopub.execute_input":"2025-03-27T16:37:58.486580Z","iopub.status.idle":"2025-03-27T16:38:00.958837Z","shell.execute_reply.started":"2025-03-27T16:37:58.486549Z","shell.execute_reply":"2025-03-27T16:38:00.957967Z"}},"outputs":[{"name":"stdout","text":"Training set size: 12491\nValidation set size: 500\n","output_type":"stream"}],"execution_count":23},{"cell_type":"markdown","source":"### Prepare the Hugging Face Dataset and Tokenizer","metadata":{}},{"cell_type":"code","source":"def preprocess_function(example):\n    # Create the input by prepending a task-specific prefix to the text.\n    input_text = \"generate title: \" + example[\"text\"]\n    target_text = example[\"title\"]\n    \n    # Tokenize inputs and targets; truncate inputs to max_length=512.\n    model_inputs = tokenizer(input_text, max_length=512, truncation=True)\n    with tokenizer.as_target_tokenizer():\n        labels = tokenizer(target_text, max_length=64, truncation=True)\n    model_inputs[\"labels\"] = labels[\"input_ids\"]\n    return model_inputs\n\n# Convert dataframes to Hugging Face Datasets\ntrain_dataset = Dataset.from_pandas(train_df)\nval_dataset = Dataset.from_pandas(val_df)\ntest_dataset = Dataset.from_pandas(test_df)\n\n# Map preprocessing over the dataset\ntrain_dataset = train_dataset.map(preprocess_function, batched=False)\nval_dataset = val_dataset.map(preprocess_function, batched=False)\ntest_dataset = test_dataset.map(preprocess_function, batched=False)\n\ndatasets = DatasetDict({\"train\": train_dataset, \"validation\": val_dataset, \"test\": test_dataset})\n\ndata_collator = DataCollatorForSeq2Seq(tokenizer, model=model)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T16:38:00.960234Z","iopub.execute_input":"2025-03-27T16:38:00.960532Z","iopub.status.idle":"2025-03-27T16:40:55.321543Z","shell.execute_reply.started":"2025-03-27T16:38:00.960486Z","shell.execute_reply":"2025-03-27T16:40:55.320350Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/12491 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"73b453289fac4602bd0495dbd96eb22a"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:3953: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1388 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1f1878afb20d4d539e4613b82c78d8e1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/100 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bd599cfc1c4d4327834f5de41dbde688"}},"metadata":{}}],"execution_count":24},{"cell_type":"markdown","source":"### Set up Training Arguments & Trainer","metadata":{}},{"cell_type":"code","source":"training_args = Seq2SeqTrainingArguments(\n    output_dir=\"./t5-finetuned-titlegen\",\n    evaluation_strategy=\"epoch\",\n    learning_rate=5e-5,\n    per_device_train_batch_size=8,\n    per_device_eval_batch_size=8,\n    weight_decay=0.01,\n    save_total_limit=3,\n    num_train_epochs=3,\n    predict_with_generate=True,\n    fp16=True,  # if using GPU with mixed precision\n    logging_dir='./logs',\n)\n\n# ROUGE metric: using the evaluate library\nrouge_metric = evaluate.load(\"rouge\")\n\ndef compute_metrics(eval_pred):\n    predictions, labels = eval_pred\n    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n    # Replace -100 in labels so they can be decoded\n    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n    # Compute ROUGE scores.\n    result = rouge_metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n    # Multiply each score by 100 to get percentages\n    result = {key: value * 100 for key, value in result.items()}\n    return result\n\ntrainer = Seq2SeqTrainer(\n    model=model,\n    args=training_args,\n    train_dataset=datasets[\"train\"],\n    eval_dataset=datasets[\"validation\"],\n    tokenizer=tokenizer,\n    data_collator=data_collator,\n    compute_metrics=compute_metrics,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T16:40:55.322741Z","iopub.execute_input":"2025-03-27T16:40:55.323130Z","iopub.status.idle":"2025-03-27T16:40:58.511553Z","shell.execute_reply.started":"2025-03-27T16:40:55.323090Z","shell.execute_reply":"2025-03-27T16:40:58.510898Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/6.27k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ce8b089b4e0a49daa7513a8c60c9a854"}},"metadata":{}},{"name":"stderr","text":"<ipython-input-25-4973d0a18e45>:30: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n  trainer = Seq2SeqTrainer(\n","output_type":"stream"}],"execution_count":25},{"cell_type":"markdown","source":"### Generate Predictions and Evaluate ROUGE","metadata":{}},{"cell_type":"code","source":"def generate_predictions(dataset, decoding_method=\"greedy\", beam_size=5):\n    generated_titles = []\n    references = []\n    for example in dataset:\n        input_ids = example[\"input_ids\"]\n        input_tensor = torch.tensor([input_ids]).to(model.device)\n        if decoding_method == \"greedy\":\n            outputs = model.generate(input_tensor, max_length=64)\n        elif decoding_method == \"beam\":\n            outputs = model.generate(input_tensor, max_length=64, num_beams=beam_size, early_stopping=True)\n        else:\n            raise ValueError(\"Unknown decoding method\")\n        pred = tokenizer.decode(outputs[0], skip_special_tokens=True)\n        generated_titles.append(pred)\n        ref = tokenizer.decode(example[\"labels\"], skip_special_tokens=True)\n        references.append(ref)\n    return generated_titles, references\n\nsample_test = test_dataset.select(range(50))\n\ngreedy_preds, greedy_refs = generate_predictions(sample_test, decoding_method=\"greedy\")\nbeam_preds, beam_refs = generate_predictions(sample_test, decoding_method=\"beam\", beam_size=5)\n\nrouge_scores_greedy = rouge_metric.compute(predictions=greedy_preds, references=greedy_refs, use_stemmer=True)\nrouge_scores_beam = rouge_metric.compute(predictions=beam_preds, references=beam_refs, use_stemmer=True)\n\nprint(\"Greedy Decoding ROUGE Scores:\")\nfor key, score in rouge_scores_greedy.items():\n    print(f\"{key}: {score*100:.2f}\")\n\nprint(\"\\nBeam Search Decoding ROUGE Scores:\")\nfor key, score in rouge_scores_beam.items():\n    print(f\"{key}: {score*100:.2f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T16:40:58.512338Z","iopub.execute_input":"2025-03-27T16:40:58.512675Z","iopub.status.idle":"2025-03-27T16:42:01.531160Z","shell.execute_reply.started":"2025-03-27T16:40:58.512639Z","shell.execute_reply":"2025-03-27T16:42:01.530169Z"}},"outputs":[{"name":"stdout","text":"Greedy Decoding ROUGE Scores:\nrouge1: 8.57\nrouge2: 2.49\nrougeL: 8.38\nrougeLsum: 8.45\n\nBeam Search Decoding ROUGE Scores:\nrouge1: 8.08\nrouge2: 2.33\nrougeL: 7.77\nrougeLsum: 7.81\n","output_type":"stream"}],"execution_count":26},{"cell_type":"markdown","source":"# C2. Prompt engineering","metadata":{}},{"cell_type":"markdown","source":"### Imports","metadata":{}},{"cell_type":"code","source":"import evaluate\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T16:42:01.532314Z","iopub.execute_input":"2025-03-27T16:42:01.532675Z","iopub.status.idle":"2025-03-27T16:42:01.536705Z","shell.execute_reply.started":"2025-03-27T16:42:01.532639Z","shell.execute_reply":"2025-03-27T16:42:01.535826Z"}},"outputs":[],"execution_count":27},{"cell_type":"markdown","source":"### Load sample dataset","metadata":{}},{"cell_type":"code","source":"# Load ROUGE metric\nrouge_metric = evaluate.load(\"rouge\")\n\n\n# Assume test CSV has columns \"text\" (article body) and \"title\" (reference title)\ntest_df = pd.read_csv(\"/kaggle/input/wiki-data/test.csv\")\n# For demonstration, select a small sample (e.g., 10 examples)\nsample_test = test_df.sample(n=10, random_state=71)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T16:42:01.538636Z","iopub.execute_input":"2025-03-27T16:42:01.538906Z","iopub.status.idle":"2025-03-27T16:42:03.387814Z","shell.execute_reply.started":"2025-03-27T16:42:01.538884Z","shell.execute_reply":"2025-03-27T16:42:03.387215Z"}},"outputs":[],"execution_count":28},{"cell_type":"markdown","source":"### Define Prompt variations","metadata":{}},{"cell_type":"code","source":"# Define two prompt variations\nprompts = [\n    \"Generate a concise title for the following article: \",\n    \"Write an appropriate, short title for this article: \"\n]\n\ndef generate_titles(model, tokenizer, article_text, prompt):\n    # Construct input by concatenating prompt and article text.\n    # Optionally, you may truncate article_text to fit model's max_length.\n    input_text = prompt + article_text\n    inputs = tokenizer(input_text, return_tensors=\"pt\", max_length=512, truncation=True)\n    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n    # Generate title with greedy decoding\n    outputs = model.generate(**inputs, max_length=64)\n    title = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    return title\n\ndef evaluate_titles(generated, references):\n    result = rouge_metric.compute(predictions=generated, references=references, use_stemmer=True)\n    # Multiply by 100 for percentages\n    result = {key: value * 100 for key, value in result.items()}\n    return result","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T16:42:03.388607Z","iopub.execute_input":"2025-03-27T16:42:03.388862Z","iopub.status.idle":"2025-03-27T16:42:03.394418Z","shell.execute_reply.started":"2025-03-27T16:42:03.388841Z","shell.execute_reply":"2025-03-27T16:42:03.393477Z"}},"outputs":[],"execution_count":29},{"cell_type":"markdown","source":"### Load Flan-T5 models","metadata":{}},{"cell_type":"code","source":"#  (base and large) along with their tokenizers\nmodel_names = {\n    \"flan-t5-base\": \"google/flan-t5-base\",\n    \"flan-t5-large\": \"google/flan-t5-large\"\n}\n\nresults = {}\n\nfor model_key, model_id in model_names.items():\n    print(f\"\\nLoading model: {model_id}\")\n    tokenizer = AutoTokenizer.from_pretrained(model_id)\n    model = AutoModelForSeq2SeqLM.from_pretrained(model_id)\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    model.to(device)\n    \n    for prompt in prompts:\n        print(f\"\\nPrompt Variation: '{prompt}'\")\n        generated_titles = []\n        reference_titles = []  # from dataset for evaluation\n        for _, row in sample_test.iterrows():\n            article_text = row[\"text\"]\n            ref_title = row[\"title\"]\n            gen_title = generate_titles(model, tokenizer, article_text, prompt)\n            generated_titles.append(gen_title)\n            reference_titles.append(ref_title)\n            print(f\"Article: {article_text[:100]}...\")\n            print(f\"Generated Title: {gen_title}\")\n            print(f\"Reference Title: {ref_title}\")\n            print(\"-\"*50)\n        \n        rouge_scores = evaluate_titles(generated_titles, reference_titles)\n        print(\"ROUGE Scores:\")\n        for key, score in rouge_scores.items():\n            print(f\"{key}: {score:.2f}\")\n        \n        # Store results for later reference\n        results[(model_key, prompt)] = {\n            \"generated_titles\": generated_titles,\n            \"reference_titles\": reference_titles,\n            \"rouge\": rouge_scores\n        }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T16:42:03.395272Z","iopub.execute_input":"2025-03-27T16:42:03.395592Z","iopub.status.idle":"2025-03-27T16:42:34.863291Z","shell.execute_reply.started":"2025-03-27T16:42:03.395563Z","shell.execute_reply":"2025-03-27T16:42:34.862612Z"}},"outputs":[{"name":"stdout","text":"\nLoading model: google/flan-t5-base\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/2.54k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4de10eff6cd94e55a5bad2285c004fe6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"16eedb4995dc48bd9007b72708c97975"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/2.42M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ace1a82a87da4646972756d53fb4e6a2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/2.20k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3f17963631264043bdeb69491bcdaa75"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.40k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4c16844f5f3249b1af91dc5aea952f14"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/990M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f4b9f465959a45ebb5f5169db158e8bd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"61104540cde54aad942934a3047fdac3"}},"metadata":{}},{"name":"stdout","text":"\nPrompt Variation: 'Generate a concise title for the following article: '\nArticle: Gottschalk or Godescalc (Old High German) is a male German name that can be translated literally as ...\nGenerated Title: Gottschalk\nReference Title: Gottschalk\n--------------------------------------------------\nArticle: Indiana Wesleyan University (IWU) is a private evangelical Christian university headquartered in Mar...\nGenerated Title: Indiana Wesleyan University\nReference Title: Indiana Wesleyan University\n--------------------------------------------------\nArticle: Abia State () is a state in the South-East geopolitical zone of Nigeria, bordered to the north and n...\nGenerated Title: Abia State\nReference Title: Abia State\n--------------------------------------------------\nArticle: Paul Thompson may refer to:\n\nEducation\nPaul Thompson (professor) (born 1951), British management pro...\nGenerated Title: Paul Thompson may also refer to:\nReference Title: Paul Thompson\n--------------------------------------------------\nArticle: The following is a list of notable people associated with Swarthmore College, a private, independent...\nGenerated Title: List of notable people associated with Swarthmore College\nReference Title: List of Swarthmore College people\n--------------------------------------------------\nArticle: Parma is a city in Canyon County, Idaho, United States. The population was 1,983 at the 2010 census,...\nGenerated Title: Parma, Idaho, is a city in Canyon County, Idaho\nReference Title: Parma, Idaho\n--------------------------------------------------\nArticle: Cheating generally describes various actions designed to subvert rules in order to obtain unfair adv...\nGenerated Title: Cheating: The New Wave in Cheating\nReference Title: Cheating\n--------------------------------------------------\nArticle: Selkirk is a town and historic royal burgh in the Scottish Borders council district of southeastern ...\nGenerated Title: Selkirk, Scotland\nReference Title: Selkirk, Scottish Borders\n--------------------------------------------------\nArticle: Ponte Vedra Beach is an unincorporated seaside community in St. Johns County, Florida, United States...\nGenerated Title: Ponte Vedra Beach, Florida\nReference Title: Ponte Vedra Beach, Florida\n--------------------------------------------------\nArticle: Rooney may refer to:\n\nPeople\nWayne Rooney, English football manager and former player\nRooney family,...\nGenerated Title: Rooney (surname)\nReference Title: Rooney\n--------------------------------------------------\nROUGE Scores:\nrouge1: 69.78\nrouge2: 38.08\nrougeL: 68.17\nrougeLsum: 67.47\n\nPrompt Variation: 'Write an appropriate, short title for this article: '\nArticle: Gottschalk or Godescalc (Old High German) is a male German name that can be translated literally as ...\nGenerated Title: Gottschalk or Godescalc\nReference Title: Gottschalk\n--------------------------------------------------\nArticle: Indiana Wesleyan University (IWU) is a private evangelical Christian university headquartered in Mar...\nGenerated Title: Indiana Wesleyan University\nReference Title: Indiana Wesleyan University\n--------------------------------------------------\nArticle: Abia State () is a state in the South-East geopolitical zone of Nigeria, bordered to the north and n...\nGenerated Title: Abia State\nReference Title: Abia State\n--------------------------------------------------\nArticle: Paul Thompson may refer to:\n\nEducation\nPaul Thompson (professor) (born 1951), British management pro...\nGenerated Title: Paul Thompson may also refer to:\nReference Title: Paul Thompson\n--------------------------------------------------\nArticle: The following is a list of notable people associated with Swarthmore College, a private, independent...\nGenerated Title: List of notable people associated with Swarthmore College\nReference Title: List of Swarthmore College people\n--------------------------------------------------\nArticle: Parma is a city in Canyon County, Idaho, United States. The population was 1,983 at the 2010 census,...\nGenerated Title: Parma, Idaho, is a city in Canyon County, Idaho\nReference Title: Parma, Idaho\n--------------------------------------------------\nArticle: Cheating generally describes various actions designed to subvert rules in order to obtain unfair adv...\nGenerated Title: Cheating: A New Wave of Cheating\nReference Title: Cheating\n--------------------------------------------------\nArticle: Selkirk is a town and historic royal burgh in the Scottish Borders council district of southeastern ...\nGenerated Title: Selkirk, Scotland\nReference Title: Selkirk, Scottish Borders\n--------------------------------------------------\nArticle: Ponte Vedra Beach is an unincorporated seaside community in St. Johns County, Florida, United States...\nGenerated Title: Ponte Vedra Beach, Florida\nReference Title: Ponte Vedra Beach, Florida\n--------------------------------------------------\nArticle: Rooney may refer to:\n\nPeople\nWayne Rooney, English football manager and former player\nRooney family,...\nGenerated Title: Rooney (surname)\nReference Title: Rooney\n--------------------------------------------------\nROUGE Scores:\nrouge1: 64.36\nrouge2: 38.08\nrougeL: 62.95\nrougeLsum: 62.44\n\nLoading model: google/flan-t5-large\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/2.54k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"13e6e448f30e4f8891ad89c7eaff8de7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a59b5a59f1a6425fafe697773a6fd845"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/2.42M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8fcfa00bcad94cd3b47fee51f6e44a28"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/2.20k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f5cf948ccaa349fe82ab06088b7d3ec8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/662 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f9746f1ccc6d46eaa928c7126dee25cb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/3.13G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d16eb2b04444401990a96528a9ace249"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"feeae33bd14c4956b01d97faacbc6df9"}},"metadata":{}},{"name":"stdout","text":"\nPrompt Variation: 'Generate a concise title for the following article: '\nArticle: Gottschalk or Godescalc (Old High German) is a male German name that can be translated literally as ...\nGenerated Title: Gottschalk\nReference Title: Gottschalk\n--------------------------------------------------\nArticle: Indiana Wesleyan University (IWU) is a private evangelical Christian university headquartered in Mar...\nGenerated Title: Indiana Wesleyan University\nReference Title: Indiana Wesleyan University\n--------------------------------------------------\nArticle: Abia State () is a state in the South-East geopolitical zone of Nigeria, bordered to the north and n...\nGenerated Title: Abia State\nReference Title: Abia State\n--------------------------------------------------\nArticle: Paul Thompson may refer to:\n\nEducation\nPaul Thompson (professor) (born 1951), British management pro...\nGenerated Title: Paul Thompson\nReference Title: Paul Thompson\n--------------------------------------------------\nArticle: The following is a list of notable people associated with Swarthmore College, a private, independent...\nGenerated Title: Swarthmore College alumni\nReference Title: List of Swarthmore College people\n--------------------------------------------------\nArticle: Parma is a city in Canyon County, Idaho, United States. The population was 1,983 at the 2010 census,...\nGenerated Title: Parma, Idaho\nReference Title: Parma, Idaho\n--------------------------------------------------\nArticle: Cheating generally describes various actions designed to subvert rules in order to obtain unfair adv...\nGenerated Title: Cheating\nReference Title: Cheating\n--------------------------------------------------\nArticle: Selkirk is a town and historic royal burgh in the Scottish Borders council district of southeastern ...\nGenerated Title: Selkirk\nReference Title: Selkirk, Scottish Borders\n--------------------------------------------------\nArticle: Ponte Vedra Beach is an unincorporated seaside community in St. Johns County, Florida, United States...\nGenerated Title: Ponte Vedra Beach, Florida\nReference Title: Ponte Vedra Beach, Florida\n--------------------------------------------------\nArticle: Rooney may refer to:\n\nPeople\nWayne Rooney, English football manager and former player\nRooney family,...\nGenerated Title: Rooney\nReference Title: Rooney\n--------------------------------------------------\nROUGE Scores:\nrouge1: 90.00\nrouge2: 53.33\nrougeL: 90.00\nrougeLsum: 90.00\n\nPrompt Variation: 'Write an appropriate, short title for this article: '\nArticle: Gottschalk or Godescalc (Old High German) is a male German name that can be translated literally as ...\nGenerated Title: Gottschalk\nReference Title: Gottschalk\n--------------------------------------------------\nArticle: Indiana Wesleyan University (IWU) is a private evangelical Christian university headquartered in Mar...\nGenerated Title: Indiana Wesleyan University\nReference Title: Indiana Wesleyan University\n--------------------------------------------------\nArticle: Abia State () is a state in the South-East geopolitical zone of Nigeria, bordered to the north and n...\nGenerated Title: Abia State\nReference Title: Abia State\n--------------------------------------------------\nArticle: Paul Thompson may refer to:\n\nEducation\nPaul Thompson (professor) (born 1951), British management pro...\nGenerated Title: Paul Thompson\nReference Title: Paul Thompson\n--------------------------------------------------\nArticle: The following is a list of notable people associated with Swarthmore College, a private, independent...\nGenerated Title: Swarthmore College alumni\nReference Title: List of Swarthmore College people\n--------------------------------------------------\nArticle: Parma is a city in Canyon County, Idaho, United States. The population was 1,983 at the 2010 census,...\nGenerated Title: Parma, Idaho\nReference Title: Parma, Idaho\n--------------------------------------------------\nArticle: Cheating generally describes various actions designed to subvert rules in order to obtain unfair adv...\nGenerated Title: Cheating: What is it?\nReference Title: Cheating\n--------------------------------------------------\nArticle: Selkirk is a town and historic royal burgh in the Scottish Borders council district of southeastern ...\nGenerated Title: Selkirk\nReference Title: Selkirk, Scottish Borders\n--------------------------------------------------\nArticle: Ponte Vedra Beach is an unincorporated seaside community in St. Johns County, Florida, United States...\nGenerated Title: Ponte Vedra Beach, Florida\nReference Title: Ponte Vedra Beach, Florida\n--------------------------------------------------\nArticle: Rooney may refer to:\n\nPeople\nWayne Rooney, English football manager and former player\nRooney family,...\nGenerated Title: Rooney\nReference Title: Rooney\n--------------------------------------------------\nROUGE Scores:\nrouge1: 84.00\nrouge2: 53.33\nrougeL: 84.00\nrougeLsum: 84.00\n","output_type":"stream"}],"execution_count":30}]}