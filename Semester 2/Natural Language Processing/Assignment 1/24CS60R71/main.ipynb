{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b0f2772",
   "metadata": {},
   "source": [
    "# A: Dataset and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "33186418",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/krishna/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/krishna/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/krishna/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importing necessary libraries\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from collections import Counter,defaultdict\n",
    "import numpy as np\n",
    "\n",
    "import re\n",
    "import time\n",
    "import math\n",
    "\n",
    "# Download necessary NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2e9edeca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading train and test data\n",
    "train_df = pd.read_csv(\"data/train.csv\")\n",
    "test_df = pd.read_csv(\"data/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e7c8cdae",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "\n",
    "# Randomly sampling 100 rows for validation set\n",
    "val_df = train_df.sample(n=100, random_state=71)\n",
    "train_df = train_df.drop(val_df.index)\n",
    "\n",
    "# Convert text column to list\n",
    "train_corpus = train_df['text'].tolist()\n",
    "test_corpus = test_df['text'].tolist()\n",
    "val_corpus = val_df['text'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "495690a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset length:  13779\n",
      "Testing dataset length:  100\n",
      "Validation dataset length:  100\n"
     ]
    }
   ],
   "source": [
    "print(\"Training dataset length: \", len(train_corpus))\n",
    "print(\"Testing dataset length: \", len(test_corpus))\n",
    "print(\"Validation dataset length: \", len(val_corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8a5260e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text Preprocessing\n",
    "def preprocess_text(text):\n",
    "    text = text.encode(\"ascii\", \"ignore\").decode()\n",
    "    text = re.sub(r'[^\\w\\s]', '', text) # Remove punctuations\n",
    "    text = text.lower() # Convert to lowercase\n",
    "    words = word_tokenize(text) # Tokenization\n",
    "    stop_words = set(stopwords.words('english')) # Remove stopwords\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "    lemmatizer = WordNetLemmatizer() # Lemmatization\n",
    "    words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "42acee14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing is done successfully!!\n",
      "Preprocessing Time: 87.10 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "train_corpus=[preprocess_text(text) for text in train_corpus]\n",
    "test_corpus=[preprocess_text(text) for text in test_corpus]\n",
    "val_corpus =[preprocess_text(text) for text in val_corpus]\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "print(f\"Preprocessing Time: {(end - start):.2f} seconds\")\n",
    "print(\"Preprocessing is done successfully!!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edbe9ad0",
   "metadata": {},
   "source": [
    "# B: Estimation using MLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "06ea57d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_ngrams(tokens, n):\n",
    "    return [tuple(tokens[i:i + n]) for i in range(len(tokens) - n + 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6f625466",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "num_docs = len(train_corpus)\n",
    "\n",
    "# Count n-grams appearing in at least 1% of documents\n",
    "min_doc_threshold = 0.01 * num_docs\n",
    "\n",
    "ngram_counts = {1: Counter(), 2: Counter(), 3: Counter()}\n",
    "doc_counts = {1: defaultdict(int), 2: defaultdict(int), 3: defaultdict(int)}\n",
    "\n",
    "for tokens in train_corpus:\n",
    "    for n in [1, 2, 3]:\n",
    "        ngrams = set(generate_ngrams(tokens, n))  # Unique n-grams in this doc\n",
    "        for ngram in ngrams:\n",
    "            doc_counts[n][ngram] += 1\n",
    "\n",
    "for n in [1, 2, 3]:\n",
    "    for ngram, count in doc_counts[n].items():\n",
    "        if count >= min_doc_threshold:\n",
    "            ngram_counts[n][ngram] = count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ddbede7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unigrams Count:  9442\n",
      "Bigrams Count:  5828\n",
      "Trigrams Count:  835\n"
     ]
    }
   ],
   "source": [
    "print(\"Unigrams Count: \", len(ngram_counts[1]))\n",
    "print(\"Bigrams Count: \", len(ngram_counts[2]))\n",
    "print(\"Trigrams Count: \", len(ngram_counts[3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1a31bfa",
   "metadata": {},
   "source": [
    "### Laplace Smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f2282ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(set(word for tokens in train_corpus for word in tokens))\n",
    "\n",
    "k = 1  # Laplace Smoothing Parameter\n",
    "\n",
    "ngram_probs = {}\n",
    "\n",
    "for n in [1, 2, 3]:\n",
    "    probabilities = {}\n",
    "    lower_order_counts = ngram_counts[n - 1] if n > 1 else {}  # Lower-order n-gram\n",
    "\n",
    "    for ngram, count in ngram_counts[n].items():\n",
    "        prefix = ngram[:-1]  # Extract (n-1)-gram prefix\n",
    "        prefix_count = lower_order_counts.get(prefix, 0) if lower_order_counts else sum(ngram_counts[n].values())\n",
    "\n",
    "        # Avoid division by zero\n",
    "        if prefix_count == 0:\n",
    "            prefix_count = 1\n",
    "\n",
    "        probabilities[ngram] = (count + k) / (prefix_count + k * vocab_size)\n",
    "\n",
    "    ngram_probs[n] = probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2b45b805",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimation of MLE Time: 35.73 seconds\n",
      "MLE estimation is done successfully!!\n"
     ]
    }
   ],
   "source": [
    "end = time.time()\n",
    "print(f\"Estimation of MLE Time: {(end - start_time):.2f} seconds\")\n",
    "print(\"MLE estimation is done successfully!!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "054c9cd6",
   "metadata": {},
   "source": [
    "# C: Evaluating an n-Gram Model using Perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7f27d022",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_perplexity(test_corpus, n, probabilities, vocab_size, k=1):\n",
    "    total_log_prob = 0\n",
    "    total_words = 0\n",
    "\n",
    "    for tokens in test_corpus:\n",
    "        ngrams = generate_ngrams(tokens, n)\n",
    "        for ngram in ngrams:\n",
    "            prob = probabilities.get(ngram, k / vocab_size)  # Use k/V for unseen n-grams\n",
    "            total_log_prob += math.log(prob)\n",
    "        total_words += len(ngrams)\n",
    "\n",
    "    return 2 ** (-total_log_prob / total_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "68d11c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "\n",
    "unigram_perplexity = calculate_perplexity(test_corpus, 1, ngram_probs[1], vocab_size, k)\n",
    "bigram_perplexity = calculate_perplexity(test_corpus, 2, ngram_probs[2], vocab_size, k)\n",
    "trigram_perplexity = calculate_perplexity(test_corpus, 3, ngram_probs[3], vocab_size, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "01919439",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mean perplexity\n",
    "avg_unigram_perplexity = np.mean(unigram_perplexity)\n",
    "avg_bigram_perplexity = np.mean(bigram_perplexity)\n",
    "avg_trigram_perplexity = np.mean(trigram_perplexity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8dad981b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Unigram Perplexity: 533.31\n",
      "Bigram Perplexity: 5346.02\n",
      "Trigram Perplexity: 8706.13\n",
      "\n",
      "Average Unigram Perplexity: 533.31\n",
      "Average Bigram Perplexity: 5346.02\n",
      "Average Trigram Perplexity: 8706.13\n",
      "\n",
      "Perplexity Calulation Time: 0.11 seconds\n",
      "Perplexity calculation is done successfully!!\n"
     ]
    }
   ],
   "source": [
    "#Print perplexity\n",
    "print(f\"\\nUnigram Perplexity: {unigram_perplexity:.2f}\")\n",
    "print(f\"Bigram Perplexity: {bigram_perplexity:.2f}\")\n",
    "print(f\"Trigram Perplexity: {trigram_perplexity:.2f}\")\n",
    "\n",
    "print(f\"\\nAverage Unigram Perplexity: {avg_unigram_perplexity:.2f}\")\n",
    "print(f\"Average Bigram Perplexity: {avg_bigram_perplexity:.2f}\")\n",
    "print(f\"Average Trigram Perplexity: {avg_trigram_perplexity:.2f}\")\n",
    "\n",
    "end = time.time()\n",
    "print(f\"\\nPerplexity Calulation Time: {(end - start):.2f} seconds\")\n",
    "print(\"Perplexity calculation is done successfully!!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1ed6cb7",
   "metadata": {},
   "source": [
    "# D: Interpolation Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9c9d18e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interpolation Model\n",
    "def interpolation_prob(w_i, w_prev2, w_prev1, lambda1, lambda2, lambda3, trigram_probs, bigram_probs, unigram_probs, vocab_size):\n",
    "    trigram_prob = trigram_probs.get((w_prev2, w_prev1, w_i), 0)  # P(w_i | w_{i-2}, w_{i-1})\n",
    "    bigram_prob = bigram_probs.get((w_prev1, w_i), 0)  # P(w_i | w_{i-1})\n",
    "    unigram_prob = unigram_probs.get(w_i, 0)  # P(w_i)\n",
    "    \n",
    "    # Apply Laplace smoothing (if the n-gram is unseen, use k/V)\n",
    "    k = 1\n",
    "    trigram_prob = (trigram_prob + k) / (bigram_probs.get((w_prev2, w_prev1), 0) + k * vocab_size) if trigram_prob == 0 else trigram_prob\n",
    "    bigram_prob = (bigram_prob + k) / (unigram_probs.get(w_prev1, 0) + k * vocab_size) if bigram_prob == 0 else bigram_prob\n",
    "    unigram_prob = (unigram_prob + k) / (len(unigram_probs) + k * vocab_size) if unigram_prob == 0 else unigram_prob\n",
    "    \n",
    "    # Combine probabilities using the interpolation weights\n",
    "    prob = lambda1 * trigram_prob + lambda2 * bigram_prob + lambda3 * unigram_prob\n",
    "    return prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "10bb1c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate perplexity using interpolation model\n",
    "def calculate_interpolation_perplexity(test_corpus, lambda1, lambda2, lambda3, trigram_probs, bigram_probs, unigram_probs, vocab_size):\n",
    "    total_log_prob = 0\n",
    "    total_words = 0\n",
    "    for tokens in test_corpus:\n",
    "        total_log_prob_article = 0\n",
    "        total_article_words = 0\n",
    "        \n",
    "        for i in range(2, len(tokens)):\n",
    "            w_i = tokens[i]\n",
    "            w_prev2, w_prev1 = tokens[i-2], tokens[i-1]\n",
    "            \n",
    "            prob = interpolation_prob(w_i, w_prev2, w_prev1, lambda1, lambda2, lambda3, trigram_probs, bigram_probs, unigram_probs, vocab_size)\n",
    "            total_log_prob_article += math.log(prob)\n",
    "            total_article_words += 1\n",
    "        \n",
    "        total_log_prob += total_log_prob_article\n",
    "        total_words += total_article_words\n",
    "        \n",
    "    return math.exp(-total_log_prob / total_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "666f9bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "\n",
    "# Tuning λ1, λ2, λ3 using validation data\n",
    "best_lambda1, best_lambda2, best_lambda3 = 0.8, 0.15, 0.05  # Start with equal weights\n",
    "best_perplexity = float('inf')\n",
    "\n",
    "lambda_values = np.linspace(0, 1, 11)  # Try values between 0 and 1 (for λ1)\n",
    "\n",
    "# Tune λ1, λ2, λ3 using grid search\n",
    "for lambda1 in lambda_values:\n",
    "    for lambda2 in lambda_values:\n",
    "        lambda3 = 1 - lambda1 - lambda2  # Ensure λ1 + λ2 + λ3 = 1\n",
    "        if lambda3 < 0: continue  # Skip invalid combinations\n",
    "        \n",
    "        # Calculate perplexity on validation data\n",
    "        perplexity = calculate_interpolation_perplexity(val_corpus, lambda1, lambda2, lambda3, ngram_probs[3], ngram_probs[2], ngram_probs[1], vocab_size)\n",
    "        \n",
    "        if perplexity < best_perplexity:\n",
    "            best_perplexity = perplexity\n",
    "            best_lambda1, best_lambda2, best_lambda3 = lambda1, lambda2, lambda3\n",
    "\n",
    "# After tuning λ values, calculate perplexity on test set\n",
    "test_perplexity = calculate_interpolation_perplexity(test_corpus, best_lambda1, best_lambda2, best_lambda3, ngram_probs[3], ngram_probs[2], ngram_probs[1], vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "449b4f4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best λ1: 0.0, λ2: 1.0, λ3: 0.0\n",
      "Test Set Perplexity with Interpolation Model: 238947.72\n",
      "\n",
      "Interpolation Model Time: 5.47 seconds\n",
      "Interpolation model is done successfully!!\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nBest λ1: {best_lambda1}, λ2: {best_lambda2}, λ3: {best_lambda3}\")\n",
    "print(f\"Test Set Perplexity with Interpolation Model: {test_perplexity:.2f}\")\n",
    "\n",
    "end = time.time()\n",
    "print(f\"\\nInterpolation Model Time: {(end - start):.2f} seconds\")\n",
    "print(\"Interpolation model is done successfully!!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_assignment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
