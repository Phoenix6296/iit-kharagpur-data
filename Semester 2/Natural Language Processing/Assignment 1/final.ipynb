{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from collections import defaultdict, Counter\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_all(df):\n",
    "    df['text'] = df['text'].apply(lambda x: x.encode(\"ascii\", \"ignore\").decode())\n",
    "    df['text'] = df['text'].apply(lambda x: re.sub(r'[^\\w\\s]', '', x))\n",
    "    df['text'] = df['text'].apply(lambda x: x.lower())\n",
    "    df['text'] = df['text'].apply(lambda x: re.sub(r'\\d+', '<NUM>', x))\n",
    "    df['text'] = df['text'].apply(lambda x: word_tokenize(x))\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    df['text'] = df['text'].apply(lambda x: [word for word in x if word not in stop_words])\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    df['text'] = df['text'].apply(lambda x: [lemmatizer.lemmatize(word) for word in x])\n",
    "    return df['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('data/train.csv')\n",
    "test_df = pd.read_csv('data/test.csv')\n",
    "\n",
    "validation_df = train_df.sample(n=100, random_state=71)\n",
    "new_train_df = train_df.drop(validation_df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_train_tokens = preprocess_all(new_train_df)\n",
    "validation_tokens = preprocess_all(validation_df)\n",
    "test_tokens = preprocess_all(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13779\n",
      "100\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "print(len(new_train_tokens))\n",
    "print(len(validation_tokens))\n",
    "print(len(test_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_count = max(1, int(len(new_train_df) * 0.01))\n",
    "\n",
    "article_counts_uni = defaultdict(int)\n",
    "for tokens in new_train_tokens:\n",
    "    unique_tokens = set(tokens)\n",
    "    for token in unique_tokens:\n",
    "        article_counts_uni[token] += 1\n",
    "\n",
    "unigram_vocab = {token for token, cnt in article_counts_uni.items() if cnt >= min_count}\n",
    "\n",
    "def replace_oov(tokens, vocab):\n",
    "    return [t if t in vocab else '<UNK>' for t in tokens]\n",
    "\n",
    "new_train_tokens = [replace_oov(t, unigram_vocab) for t in new_train_tokens]\n",
    "validation_tokens = [replace_oov(t, unigram_vocab) for t in validation_tokens]\n",
    "test_tokens = [replace_oov(t, unigram_vocab) for t in test_tokens]\n",
    "\n",
    "def build_ngram_vocab(tokenized_texts, n, min_count):\n",
    "    article_counts = defaultdict(int)\n",
    "    \n",
    "    for tokens in tokenized_texts:\n",
    "        ngrams = set(zip(*[tokens[i:] for i in range(n)]))  # Generate unique n-grams\n",
    "        for ngram in ngrams:\n",
    "            article_counts[ngram] += 1\n",
    "    \n",
    "    ngram_vocab = {ngram for ngram, count in article_counts.items() if count >= min_count}\n",
    "    return ngram_vocab\n",
    "\n",
    "bigram_vocab = build_ngram_vocab(new_train_tokens, n=2, min_count=min_count)\n",
    "trigram_vocab = build_ngram_vocab(new_train_tokens, n=3, min_count=min_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8590\n",
      "6789\n",
      "3782\n"
     ]
    }
   ],
   "source": [
    "print(len(unigram_vocab))\n",
    "print(len(bigram_vocab))\n",
    "print(len(trigram_vocab))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_assignment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
