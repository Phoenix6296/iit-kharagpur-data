{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7fa02ed2-d79b-43fe-af34-af21cb84b272",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from collections import defaultdict, Counter\n",
    "import math\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "49dc7319-de77-41d4-a4ad-874445c1c31e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def remove_punct_and_non_ascii(text):\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    return text.encode('ascii', 'ignore').decode('ascii')\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\d+', '<NUM>', text)\n",
    "    return text\n",
    "\n",
    "def tokenize(text):\n",
    "    return word_tokenize(text)\n",
    "\n",
    "def remove_stopwords(tokens):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    return [t for t in tokens if t not in stop_words]\n",
    "\n",
    "def stem_tokens(tokens):\n",
    "    stemmer = PorterStemmer()\n",
    "    return [stemmer.stem(t) for t in tokens]\n",
    "\n",
    "def preprocess_pipeline(text):\n",
    "    text = remove_punct_and_non_ascii(text)\n",
    "    text = preprocess_text(text)\n",
    "    tokens = tokenize(text)\n",
    "    tokens = remove_stopwords(tokens)\n",
    "    tokens = stem_tokens(tokens)\n",
    "    return tokens\n",
    "\n",
    "train_df = pd.read_csv('data/train.csv')\n",
    "test_df = pd.read_csv('data/test.csv')\n",
    "\n",
    "validation_df = train_df.sample(n=100, random_state=42)\n",
    "new_train_df = train_df.drop(validation_df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e9912944-b535-4cb1-9749-b17727f38430",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_all(df):\n",
    "    return [preprocess_pipeline(text) for text in df['text']]\n",
    "\n",
    "new_train_tokens = preprocess_all(new_train_df)\n",
    "validation_tokens = preprocess_all(validation_df)\n",
    "test_tokens = preprocess_all(test_df)\n",
    "\n",
    "min_count = max(1, int(len(new_train_df) * 0.01))\n",
    "\n",
    "article_counts_uni = defaultdict(int)\n",
    "for tokens in new_train_tokens:\n",
    "    unique_tokens = set(tokens)\n",
    "    for token in unique_tokens:\n",
    "        article_counts_uni[token] += 1\n",
    "\n",
    "unigram_vocab = {token for token, cnt in article_counts_uni.items() if cnt >= min_count}\n",
    "\n",
    "def replace_oov(tokens, vocab):\n",
    "    return [t if t in vocab else '<UNK>' for t in tokens]\n",
    "\n",
    "new_train_tokens = [replace_oov(t, unigram_vocab) for t in new_train_tokens]\n",
    "validation_tokens = [replace_oov(t, unigram_vocab) for t in validation_tokens]\n",
    "test_tokens = [replace_oov(t, unigram_vocab) for t in test_tokens]\n",
    "\n",
    "article_counts_bi = defaultdict(int)\n",
    "for tokens in new_train_tokens:\n",
    "    bigrams = set(zip(tokens[:-1], tokens[1:]))\n",
    "    for bg in bigrams:\n",
    "        article_counts_bi[bg] += 1\n",
    "\n",
    "bigram_vocab = {bg for bg, cnt in article_counts_bi.items() if cnt >= min_count}\n",
    "\n",
    "article_counts_tri = defaultdict(int)\n",
    "for tokens in new_train_tokens:\n",
    "    trigrams = set(zip(tokens[:-2], tokens[1:-1], tokens[2:]))\n",
    "    for tg in trigrams:\n",
    "        article_counts_tri[tg] += 1\n",
    "\n",
    "trigram_vocab = {tg for tg, cnt in article_counts_tri.items() if cnt >= min_count}\n",
    "\n",
    "V = len(unigram_vocab)\n",
    "k = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dcc92d28-13ce-4986-b54b-6b3511b2e6d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Unigram model\n",
    "unigram_counts = Counter()\n",
    "for tokens in new_train_tokens:\n",
    "    unigram_counts.update(tokens)\n",
    "\n",
    "N_uni = sum(unigram_counts.values())\n",
    "unigram_probs = {token: (unigram_counts[token] + k) / (N_uni + k * V) for token in unigram_vocab}\n",
    "\n",
    "# Bigram model\n",
    "bigram_counts = Counter()\n",
    "for tokens in new_train_tokens:\n",
    "    bigrams = list(zip(tokens[:-1], tokens[1:]))\n",
    "    bigram_counts.update(bigrams)\n",
    "\n",
    "bigram_probs = {}\n",
    "for bg in bigram_vocab:\n",
    "    prev, curr = bg\n",
    "    count_bg = bigram_counts[bg]\n",
    "    count_prev = unigram_counts.get(prev, 0)\n",
    "    bigram_probs[bg] = (count_bg + k) / (count_prev + k * V)\n",
    "\n",
    "# Trigram model\n",
    "trigram_counts = Counter()\n",
    "for tokens in new_train_tokens:\n",
    "    trigrams = list(zip(tokens[:-2], tokens[1:-1], tokens[2:]))\n",
    "    trigram_counts.update(trigrams)\n",
    "\n",
    "bigram_context_counts = Counter()\n",
    "for tokens in new_train_tokens:\n",
    "    bigrams_ctx = list(zip(tokens[:-1], tokens[1:]))\n",
    "    bigram_context_counts.update(bigrams_ctx)\n",
    "\n",
    "trigram_probs = {}\n",
    "for tg in trigram_vocab:\n",
    "    prev_prev, prev, curr = tg\n",
    "    ctx = (prev_prev, prev)\n",
    "    count_tg = trigram_counts[tg]\n",
    "    count_ctx = bigram_context_counts.get(ctx, 0)\n",
    "    trigram_probs[tg] = (count_tg + k) / (count_ctx + k * V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1fce1bb2-7963-4e18-8ddd-fb8140bbeb6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate perplexity\n",
    "def calculate_perplexity(model_probs, tokens, model_type):\n",
    "    log_sum = 0\n",
    "    T = 0\n",
    "    if model_type == 'unigram':\n",
    "        for t in tokens:\n",
    "            prob = unigram_probs.get(t, 1 / (N_uni + V))\n",
    "            log_sum += math.log(prob)\n",
    "        T = len(tokens)\n",
    "    elif model_type == 'bigram':\n",
    "        for i in range(1, len(tokens)):\n",
    "            bg = (tokens[i-1], tokens[i])\n",
    "            prob = bigram_probs.get(bg, 1 / (unigram_counts.get(tokens[i-1], 0) + V))\n",
    "            log_sum += math.log(prob)\n",
    "        T = len(tokens) - 1\n",
    "    elif model_type == 'trigram':\n",
    "        for i in range(2, len(tokens)):\n",
    "            tg = (tokens[i-2], tokens[i-1], tokens[i])\n",
    "            prob = trigram_probs.get(tg, 1 / (bigram_context_counts.get((tokens[i-2], tokens[i-1]), 0) + V))\n",
    "            log_sum += math.log(prob)\n",
    "        T = len(tokens) - 2\n",
    "    else:\n",
    "        raise ValueError(\"Invalid model type\")\n",
    "    return math.exp(-log_sum / T) if T > 0 else float('inf')\n",
    "\n",
    "def evaluate_perplexity(model_type):\n",
    "    perplexities = []\n",
    "    for tokens in test_tokens:\n",
    "        perplexities.append(calculate_perplexity(None, tokens, model_type))\n",
    "    return np.mean(perplexities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "735454bb-ee6b-4190-9cde-29b84ce76a0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unigram Perplexity: 4289.919588189338\n",
      "Bigram Perplexity: 683.1776678821734\n",
      "Trigram Perplexity: 1551.98516427819\n"
     ]
    }
   ],
   "source": [
    "uni_ppl = evaluate_perplexity('unigram')\n",
    "bi_ppl = evaluate_perplexity('bigram')\n",
    "tri_ppl = evaluate_perplexity('trigram')\n",
    "\n",
    "print(f\"Unigram Perplexity: {uni_ppl}\")\n",
    "print(f\"Bigram Perplexity: {bi_ppl}\")\n",
    "print(f\"Trigram Perplexity: {tri_ppl}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e9e814a5-0c99-409a-a9e9-5917baf129e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Lambdas: (np.float64(0.0), np.float64(1.0), np.float64(0.0))\n",
      "Interpolated Perplexity: 381.1258730408566\n"
     ]
    }
   ],
   "source": [
    "# Interpolation model\n",
    "def interpolated_prob(tokens, lambdas):\n",
    "    l1, l2, l3 = lambdas\n",
    "    total_log = 0\n",
    "    for i in range(len(tokens)):\n",
    "        uni_prob = unigram_probs.get(tokens[i], 1 / (N_uni + V))\n",
    "        bg_prob = bigram_probs.get((tokens[i-1], tokens[i]), uni_prob) if i > 0 else uni_prob\n",
    "        tg_prob = trigram_probs.get((tokens[i-2], tokens[i-1], tokens[i]), bg_prob) if i > 1 else bg_prob\n",
    "        prob = l1 * tg_prob + l2 * bg_prob + l3 * uni_prob\n",
    "        total_log += math.log(prob)\n",
    "    return math.exp(-total_log / len(tokens))\n",
    "\n",
    "best_ppl = float('inf')\n",
    "best_lambdas = (0.4, 0.3, 0.3)\n",
    "\n",
    "for l1 in np.linspace(0, 1, 11):\n",
    "    for l2 in np.linspace(0, 1 - l1, 11):\n",
    "        l3 = 1 - l1 - l2\n",
    "        if l3 < 0:\n",
    "            continue\n",
    "        avg_ppl = np.mean([interpolated_prob(t, (l1, l2, l3)) for t in validation_tokens])\n",
    "        if avg_ppl < best_ppl:\n",
    "            best_ppl = avg_ppl\n",
    "            best_lambdas = (l1, l2, l3)\n",
    "\n",
    "interp_ppl = np.mean([interpolated_prob(t, best_lambdas) for t in test_tokens])\n",
    "\n",
    "print(f\"Best Lambdas: {best_lambdas}\")\n",
    "print(f\"Interpolated Perplexity: {interp_ppl}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_assignment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
