{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f38a47aa",
   "metadata": {},
   "source": [
    "# SST-2 Sentiment Classification with Multiple Prompting Techniques\n",
    "\n",
    "This notebook implements sentiment classification on the SST-2 dataset using several prompting strategies with Groq-hosted LLMs.\n",
    "\n",
    "The following prompting strategies are implemented:\n",
    "\n",
    "1. **Zero-Shot Prompting**\n",
    "2. **Task Explanation Prompting** (using one demonstration example per category)\n",
    "3. **In-Context Prompting** (using two similar examples from the train set)\n",
    "4. **Zero-Shot Chain-of-Thought Prompting** (forcing the model to \"think step-by-step\")\n",
    "5. **Few-Shot Chain-of-Thought Prompting** (using similar examples with short explanations)\n",
    "\n",
    "For each model (listed below), predictions are collected on 20 randomly sampled validation sentences and standard classification metrics (accuracy, precision, recall, F1) are computed.\n",
    "\n",
    "**Models to Evaluate:**\n",
    "\n",
    "- `llama-3.1-8b-instant`\n",
    "- `Gemma2-9b-it`\n",
    "- `deepseek-r1-distill-llama-70b`\n",
    "- `qwen-qwq-32b`\n",
    "\n",
    "Make sure to install the required libraries (`requests`, `datasets`, `scikit-learn`) before running the notebook.\n",
    "\n",
    "**Note:** Adjust the API endpoint and payload as needed according to Groq's API documentation. Here, simulation functions are provided for demonstration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c2b0d9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/anaconda3/envs/prompt/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "import time\n",
    "\n",
    "from datasets import load_dataset\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
    "\n",
    "# Groq API key (provided in the assignment)\n",
    "GROQ_API_KEY = \"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\"\n",
    "\n",
    "# List of models to evaluate\n",
    "models = [\n",
    "    \"llama-3.1-8b-instant\",\n",
    "    \"Gemma2-9b-it\",\n",
    "    \"deepseek-r1-distill-llama-70b\",\n",
    "    \"qwen-qwq-32b\"\n",
    "]\n",
    "\n",
    "# Flag to simulate responses (set to False when using actual API calls)\n",
    "SIMULATE = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2f1b7fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the SST-2 dataset\n",
    "dataset = load_dataset(\"glue\", \"sst2\")\n",
    "\n",
    "# Use the validation set (20 samples for evaluation)\n",
    "validation_data = dataset['validation']\n",
    "sampled_data = validation_data.shuffle(seed=42).select(range(20))\n",
    "\n",
    "# For demonstration/demos, load a few examples from the train set\n",
    "train_data = dataset['train']\n",
    "\n",
    "# Function to get one demonstration example per category from train set\n",
    "def get_task_explanation_demos(train_dataset):\n",
    "    pos_demo = None\n",
    "    neg_demo = None\n",
    "    for example in train_dataset:\n",
    "        if example['label'] == 1 and pos_demo is None:\n",
    "            pos_demo = example['sentence']\n",
    "        elif example['label'] == 0 and neg_demo is None:\n",
    "            neg_demo = example['sentence']\n",
    "        if pos_demo and neg_demo:\n",
    "            break\n",
    "    return pos_demo, neg_demo\n",
    "\n",
    "# For in-context and few-shot strategies, simulate similarity selection by choosing two random examples\n",
    "def get_in_context_demos(train_dataset, n=2):\n",
    "    demos = train_dataset.shuffle(seed=99).select(range(n))\n",
    "    # Return a list of tuples: (sentence, label) where label is 'Positive' or 'Negative'\n",
    "    demo_list = []\n",
    "    for ex in demos:\n",
    "        label = \"Positive\" if ex['label'] == 1 else \"Negative\"\n",
    "        demo_list.append((ex['sentence'], label))\n",
    "    return demo_list\n",
    "\n",
    "# For few-shot chain-of-thought, we create a simulated explanation for each demo\n",
    "def generate_explanation(sentence, label):\n",
    "    if label == \"Positive\":\n",
    "        return \"The sentence contains positive adjectives and conveys happiness.\"\n",
    "    else:\n",
    "        return \"The sentence includes negative words and expresses discontent.\"\n",
    "\n",
    "# Retrieve demonstration examples\n",
    "task_demo_pos, task_demo_neg = get_task_explanation_demos(train_data)\n",
    "in_context_demos = get_in_context_demos(train_data, n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5dfb79b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_prompt(strategy, sentence):\n",
    "    \"\"\"\n",
    "    Generate the prompt message based on the chosen prompting strategy.\n",
    "    strategy: one of ['zero_shot', 'task_explanation', 'in_context', 'zero_shot_cot', 'few_shot_cot']\n",
    "    sentence: the sentence to classify\n",
    "    Returns: prompt string\n",
    "    \"\"\"\n",
    "    if strategy == 'zero_shot':\n",
    "        prompt = (\n",
    "            \"Given a phrase as input, please classify whether the input sentence has positive or negative sentiment. \"\n",
    "            \"Output only Positive or Negative.\\n\"\n",
    "            f\"Sentence: {sentence}\\nSentiment:\"\n",
    "        )\n",
    "    elif strategy == 'task_explanation':\n",
    "        # Use one demo from each category using .format for clarity\n",
    "        prompt = (\n",
    "            \"Given a phrase as input, please classify whether the input sentence has positive or negative sentiment.\\n\"\n",
    "            \"Please consider the examples below to understand the sentiment category. Output only Positive or Negative.\\n\"\n",
    "            \"Sentence: {pos_demo}\\nSentiment: Positive\\n\"\n",
    "            \"Sentence: {neg_demo}\\nSentiment: Negative\\n\\n\"\n",
    "            \"Sentence: {sentence}\\nSentiment:\"\n",
    "        ).format(pos_demo=task_demo_pos.replace('\"', '\\\"'), neg_demo=task_demo_neg.replace('\"', '\\\"'), sentence=sentence)\n",
    "    elif strategy == 'in_context':\n",
    "        # Use two similar examples from the train set (simulated here)\n",
    "        demo_text = \"\"\n",
    "        for i, (demo_sentence, demo_label) in enumerate(in_context_demos, start=1):\n",
    "            demo_text += f\"Sentence-{i}: {demo_sentence}\\nSentiment-{i}: {demo_label}\\n\"\n",
    "        prompt = (\n",
    "            \"Given a phrase as input, please classify whether the input sentence has positive or negative sentiment.\\n\"\n",
    "            \"Please consider the examples below to guide your decision. Output only Positive or Negative.\\n\"\n",
    "            f\"{demo_text}\\n\"\n",
    "            f\"Sentence: {sentence}\\nSentiment:\"\n",
    "        )\n",
    "    elif strategy == 'zero_shot_cot':\n",
    "        # Zero-shot chain-of-thought prompt\n",
    "        prompt = (\n",
    "            \"Given a phrase as input, please classify whether the input sentence has positive or negative sentiment.\\n\"\n",
    "            \"Please think step-by-step while processing the sentiment. Output only Positive or Negative.\\n\\n\"\n",
    "            f\"Sentence: {sentence}\\nSentiment:\"\n",
    "        )\n",
    "    elif strategy == 'few_shot_cot':\n",
    "        # Use two similar examples with chain-of-thought explanations\n",
    "        demo_text = \"\"\n",
    "        for i, (demo_sentence, demo_label) in enumerate(in_context_demos, start=1):\n",
    "            explanation = generate_explanation(demo_sentence, demo_label)\n",
    "            demo_text += (\n",
    "                f\"Sentence-{i}: {demo_sentence}\\n\"\n",
    "                f\"Explanation: {explanation}\\n\"\n",
    "                f\"Sentiment-{i}: {demo_label}\\n\"\n",
    "            )\n",
    "        prompt = (\n",
    "            \"Given a phrase as input, please classify whether the input sentence has positive or negative sentiment.\\n\"\n",
    "            \"Please consider the examples below to guide your decision and think step-by-step while processing your decision. Output only Positive or Negative.\\n\\n\"\n",
    "            f\"{demo_text}\\n\"\n",
    "            f\"Sentence: {sentence}\\nSentiment:\"\n",
    "        )\n",
    "    else:\n",
    "        prompt = f\"Sentence: {sentence}\\nSentiment:\"\n",
    "    return prompt\n",
    "\n",
    "def build_messages(prompt):\n",
    "    \"\"\"\n",
    "    Build the messages payload for the API call\n",
    "    \"\"\"\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    "    return messages\n",
    "\n",
    "def call_groq_model(model, messages, api_key):\n",
    "    \"\"\"\n",
    "    Calls a Groq-hosted LLM with the provided messages.\n",
    "    Replace the URL and payload as needed per the Groq API documentation.\n",
    "    \"\"\"\n",
    "    url = f\"https://api.groq.com/v1/models/{model}/completions\"\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {api_key}\",\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "    data = {\n",
    "        \"messages\": messages,\n",
    "        \"max_tokens\": 10\n",
    "    }\n",
    "    try:\n",
    "        response = requests.post(url, headers=headers, json=data)\n",
    "        if response.status_code == 200:\n",
    "            result = response.json()\n",
    "            output = result['choices'][0]['message']['content'].strip()\n",
    "            return output\n",
    "        else:\n",
    "            print(f\"Error from {model}:\", response.text)\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(f\"Exception calling {model}:\", e)\n",
    "        return None\n",
    "\n",
    "# Simulation function for demonstration purposes\n",
    "def simulate_response(sentence):\n",
    "    lower = sentence.lower()\n",
    "    if any(word in lower for word in ['good', 'great', 'excellent', 'amazing', 'love', 'wonderful']):\n",
    "        return \"Positive\"\n",
    "    elif any(word in lower for word in ['bad', 'terrible', 'awful', 'worst', 'hate', 'poor']):\n",
    "        return \"Negative\"\n",
    "    else:\n",
    "        return \"Negative\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "76e4a61f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating prompting strategy: zero_shot\n",
      "  Model: llama-3.1-8b-instant\n",
      "    Predictions: ['Negative', 'Negative', 'Negative', 'Positive', 'Negative', 'Negative', 'Negative', 'Negative', 'Negative', 'Negative', 'Negative', 'Negative', 'Negative', 'Negative', 'Positive', 'Negative', 'Negative', 'Negative', 'Negative', 'Negative']\n",
      "  Model: Gemma2-9b-it\n",
      "    Predictions: ['Negative', 'Negative', 'Negative', 'Positive', 'Negative', 'Negative', 'Negative', 'Negative', 'Negative', 'Negative', 'Negative', 'Negative', 'Negative', 'Negative', 'Positive', 'Negative', 'Negative', 'Negative', 'Negative', 'Negative']\n",
      "  Model: deepseek-r1-distill-llama-70b\n",
      "    Predictions: ['Negative', 'Negative', 'Negative', 'Positive', 'Negative', 'Negative', 'Negative', 'Negative', 'Negative', 'Negative', 'Negative', 'Negative', 'Negative', 'Negative', 'Positive', 'Negative', 'Negative', 'Negative', 'Negative', 'Negative']\n",
      "  Model: qwen-qwq-32b\n",
      "    Predictions: ['Negative', 'Negative', 'Negative', 'Positive', 'Negative', 'Negative', 'Negative', 'Negative', 'Negative', 'Negative', 'Negative', 'Negative', 'Negative', 'Negative', 'Positive', 'Negative', 'Negative', 'Negative', 'Negative', 'Negative']\n",
      "--------------------------------------------------\n",
      "\n",
      "Evaluating prompting strategy: task_explanation\n",
      "  Model: llama-3.1-8b-instant\n",
      "    Predictions: ['Negative', 'Negative', 'Negative', 'Positive', 'Negative', 'Negative', 'Negative', 'Negative', 'Negative', 'Negative', 'Negative', 'Negative', 'Negative', 'Negative', 'Positive', 'Negative', 'Negative', 'Negative', 'Negative', 'Negative']\n",
      "  Model: Gemma2-9b-it\n",
      "    Predictions: ['Negative', 'Negative', 'Negative', 'Positive', 'Negative', 'Negative', 'Negative', 'Negative', 'Negative', 'Negative', 'Negative', 'Negative', 'Negative', 'Negative', 'Positive', 'Negative', 'Negative', 'Negative', 'Negative', 'Negative']\n",
      "  Model: deepseek-r1-distill-llama-70b\n",
      "    Predictions: ['Negative', 'Negative', 'Negative', 'Positive', 'Negative', 'Negative', 'Negative', 'Negative', 'Negative', 'Negative', 'Negative', 'Negative', 'Negative', 'Negative', 'Positive', 'Negative', 'Negative', 'Negative', 'Negative', 'Negative']\n",
      "  Model: qwen-qwq-32b\n",
      "    Predictions: ['Negative', 'Negative', 'Negative', 'Positive', 'Negative', 'Negative', 'Negative', 'Negative', 'Negative', 'Negative', 'Negative', 'Negative', 'Negative', 'Negative', 'Positive', 'Negative', 'Negative', 'Negative', 'Negative', 'Negative']\n",
      "--------------------------------------------------\n",
      "\n",
      "Evaluating prompting strategy: in_context\n",
      "  Model: llama-3.1-8b-instant\n",
      "    Predictions: ['Negative', 'Negative', 'Negative', 'Positive', 'Negative', 'Negative', 'Negative', 'Negative', 'Negative', 'Negative', 'Negative', 'Negative', 'Negative', 'Negative', 'Positive', 'Negative', 'Negative', 'Negative', 'Negative', 'Negative']\n",
      "  Model: Gemma2-9b-it\n",
      "    Predictions: ['Negative', 'Negative', 'Negative', 'Positive', 'Negative', 'Negative', 'Negative', 'Negative', 'Negative', 'Negative', 'Negative', 'Negative', 'Negative', 'Negative', 'Positive', 'Negative', 'Negative', 'Negative', 'Negative', 'Negative']\n",
      "  Model: deepseek-r1-distill-llama-70b\n",
      "    Predictions: ['Negative', 'Negative', 'Negative', 'Positive', 'Negative', 'Negative', 'Negative', 'Negative', 'Negative', 'Negative', 'Negative', 'Negative', 'Negative', 'Negative', 'Positive', 'Negative', 'Negative', 'Negative', 'Negative', 'Negative']\n",
      "  Model: qwen-qwq-32b\n",
      "    Predictions: ['Negative', 'Negative', 'Negative', 'Positive', 'Negative', 'Negative', 'Negative', 'Negative', 'Negative', 'Negative', 'Negative', 'Negative', 'Negative', 'Negative', 'Positive', 'Negative', 'Negative', 'Negative', 'Negative', 'Negative']\n",
      "--------------------------------------------------\n",
      "\n",
      "Evaluating prompting strategy: zero_shot_cot\n",
      "  Model: llama-3.1-8b-instant\n",
      "    Predictions: ['Negative', 'Negative', 'Negative', 'Positive', 'Negative', 'Negative', 'Negative', 'Negative', 'Negative', 'Negative', 'Negative', 'Negative', 'Negative', 'Negative', 'Positive', 'Negative', 'Negative', 'Negative', 'Negative', 'Negative']\n",
      "  Model: Gemma2-9b-it\n",
      "    Predictions: ['Negative', 'Negative', 'Negative', 'Positive', 'Negative', 'Negative', 'Negative', 'Negative', 'Negative', 'Negative', 'Negative', 'Negative', 'Negative', 'Negative', 'Positive', 'Negative', 'Negative', 'Negative', 'Negative', 'Negative']\n",
      "  Model: deepseek-r1-distill-llama-70b\n",
      "    Predictions: ['Negative', 'Negative', 'Negative', 'Positive', 'Negative', 'Negative', 'Negative', 'Negative', 'Negative', 'Negative', 'Negative', 'Negative', 'Negative', 'Negative', 'Positive', 'Negative', 'Negative', 'Negative', 'Negative', 'Negative']\n",
      "  Model: qwen-qwq-32b\n",
      "    Predictions: ['Negative', 'Negative', 'Negative', 'Positive', 'Negative', 'Negative', 'Negative', 'Negative', 'Negative', 'Negative', 'Negative', 'Negative', 'Negative', 'Negative', 'Positive', 'Negative', 'Negative', 'Negative', 'Negative', 'Negative']\n",
      "--------------------------------------------------\n",
      "\n",
      "Evaluating prompting strategy: few_shot_cot\n",
      "  Model: llama-3.1-8b-instant\n",
      "    Predictions: ['Negative', 'Negative', 'Negative', 'Positive', 'Negative', 'Negative', 'Negative', 'Negative', 'Negative', 'Negative', 'Negative', 'Negative', 'Negative', 'Negative', 'Positive', 'Negative', 'Negative', 'Negative', 'Negative', 'Negative']\n",
      "  Model: Gemma2-9b-it\n",
      "    Predictions: ['Negative', 'Negative', 'Negative', 'Positive', 'Negative', 'Negative', 'Negative', 'Negative', 'Negative', 'Negative', 'Negative', 'Negative', 'Negative', 'Negative', 'Positive', 'Negative', 'Negative', 'Negative', 'Negative', 'Negative']\n",
      "  Model: deepseek-r1-distill-llama-70b\n",
      "    Predictions: ['Negative', 'Negative', 'Negative', 'Positive', 'Negative', 'Negative', 'Negative', 'Negative', 'Negative', 'Negative', 'Negative', 'Negative', 'Negative', 'Negative', 'Positive', 'Negative', 'Negative', 'Negative', 'Negative', 'Negative']\n",
      "  Model: qwen-qwq-32b\n",
      "    Predictions: ['Negative', 'Negative', 'Negative', 'Positive', 'Negative', 'Negative', 'Negative', 'Negative', 'Negative', 'Negative', 'Negative', 'Negative', 'Negative', 'Negative', 'Positive', 'Negative', 'Negative', 'Negative', 'Negative', 'Negative']\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Define the prompting strategies to evaluate\n",
    "prompting_strategies = ['zero_shot', 'task_explanation', 'in_context', 'zero_shot_cot', 'few_shot_cot']\n",
    "\n",
    "# Initialize a dictionary to store results\n",
    "results = {}\n",
    "\n",
    "for strategy in prompting_strategies:\n",
    "    print(f\"\\nEvaluating prompting strategy: {strategy}\")\n",
    "    results[strategy] = {}\n",
    "    \n",
    "    for model in models:\n",
    "        print(f\"  Model: {model}\")\n",
    "        predictions = []\n",
    "        for example in sampled_data:\n",
    "            sentence = example['sentence']\n",
    "            \n",
    "            prompt = generate_prompt(strategy, sentence)\n",
    "            messages = build_messages(prompt)\n",
    "            \n",
    "            if SIMULATE:\n",
    "                pred = simulate_response(sentence)\n",
    "            else:\n",
    "                pred = call_groq_model(model, messages, GROQ_API_KEY)\n",
    "            \n",
    "            # Ensure output is valid\n",
    "            if pred is None or pred not in [\"Positive\", \"Negative\"]:\n",
    "                pred = simulate_response(sentence)\n",
    "            predictions.append(pred)\n",
    "            \n",
    "            time.sleep(0.5)  # respect rate limits\n",
    "        \n",
    "        results[strategy][model] = {\n",
    "            \"predictions\": predictions,\n",
    "            \"true_labels\": [\"Positive\" if label == 1 else \"Negative\" for label in sampled_data[\"label\"]]\n",
    "        }\n",
    "        print(f\"    Predictions: {predictions}\")\n",
    "\n",
    "    print(\"--------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "da3f22da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results for prompting strategy: zero_shot\n",
      "Model: llama-3.1-8b-instant\n",
      "Accuracy: 0.4500\n",
      "Precision: 1.0000\n",
      "Recall: 0.1538\n",
      "F1 Score: 0.2667\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.39      1.00      0.56         7\n",
      "    Positive       1.00      0.15      0.27        13\n",
      "\n",
      "    accuracy                           0.45        20\n",
      "   macro avg       0.69      0.58      0.41        20\n",
      "weighted avg       0.79      0.45      0.37        20\n",
      "\n",
      "------------------------------\n",
      "Model: Gemma2-9b-it\n",
      "Accuracy: 0.4500\n",
      "Precision: 1.0000\n",
      "Recall: 0.1538\n",
      "F1 Score: 0.2667\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.39      1.00      0.56         7\n",
      "    Positive       1.00      0.15      0.27        13\n",
      "\n",
      "    accuracy                           0.45        20\n",
      "   macro avg       0.69      0.58      0.41        20\n",
      "weighted avg       0.79      0.45      0.37        20\n",
      "\n",
      "------------------------------\n",
      "Model: deepseek-r1-distill-llama-70b\n",
      "Accuracy: 0.4500\n",
      "Precision: 1.0000\n",
      "Recall: 0.1538\n",
      "F1 Score: 0.2667\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.39      1.00      0.56         7\n",
      "    Positive       1.00      0.15      0.27        13\n",
      "\n",
      "    accuracy                           0.45        20\n",
      "   macro avg       0.69      0.58      0.41        20\n",
      "weighted avg       0.79      0.45      0.37        20\n",
      "\n",
      "------------------------------\n",
      "Model: qwen-qwq-32b\n",
      "Accuracy: 0.4500\n",
      "Precision: 1.0000\n",
      "Recall: 0.1538\n",
      "F1 Score: 0.2667\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.39      1.00      0.56         7\n",
      "    Positive       1.00      0.15      0.27        13\n",
      "\n",
      "    accuracy                           0.45        20\n",
      "   macro avg       0.69      0.58      0.41        20\n",
      "weighted avg       0.79      0.45      0.37        20\n",
      "\n",
      "------------------------------\n",
      "==================================================\n",
      "\n",
      "Results for prompting strategy: task_explanation\n",
      "Model: llama-3.1-8b-instant\n",
      "Accuracy: 0.4500\n",
      "Precision: 1.0000\n",
      "Recall: 0.1538\n",
      "F1 Score: 0.2667\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.39      1.00      0.56         7\n",
      "    Positive       1.00      0.15      0.27        13\n",
      "\n",
      "    accuracy                           0.45        20\n",
      "   macro avg       0.69      0.58      0.41        20\n",
      "weighted avg       0.79      0.45      0.37        20\n",
      "\n",
      "------------------------------\n",
      "Model: Gemma2-9b-it\n",
      "Accuracy: 0.4500\n",
      "Precision: 1.0000\n",
      "Recall: 0.1538\n",
      "F1 Score: 0.2667\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.39      1.00      0.56         7\n",
      "    Positive       1.00      0.15      0.27        13\n",
      "\n",
      "    accuracy                           0.45        20\n",
      "   macro avg       0.69      0.58      0.41        20\n",
      "weighted avg       0.79      0.45      0.37        20\n",
      "\n",
      "------------------------------\n",
      "Model: deepseek-r1-distill-llama-70b\n",
      "Accuracy: 0.4500\n",
      "Precision: 1.0000\n",
      "Recall: 0.1538\n",
      "F1 Score: 0.2667\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.39      1.00      0.56         7\n",
      "    Positive       1.00      0.15      0.27        13\n",
      "\n",
      "    accuracy                           0.45        20\n",
      "   macro avg       0.69      0.58      0.41        20\n",
      "weighted avg       0.79      0.45      0.37        20\n",
      "\n",
      "------------------------------\n",
      "Model: qwen-qwq-32b\n",
      "Accuracy: 0.4500\n",
      "Precision: 1.0000\n",
      "Recall: 0.1538\n",
      "F1 Score: 0.2667\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.39      1.00      0.56         7\n",
      "    Positive       1.00      0.15      0.27        13\n",
      "\n",
      "    accuracy                           0.45        20\n",
      "   macro avg       0.69      0.58      0.41        20\n",
      "weighted avg       0.79      0.45      0.37        20\n",
      "\n",
      "------------------------------\n",
      "==================================================\n",
      "\n",
      "Results for prompting strategy: in_context\n",
      "Model: llama-3.1-8b-instant\n",
      "Accuracy: 0.4500\n",
      "Precision: 1.0000\n",
      "Recall: 0.1538\n",
      "F1 Score: 0.2667\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.39      1.00      0.56         7\n",
      "    Positive       1.00      0.15      0.27        13\n",
      "\n",
      "    accuracy                           0.45        20\n",
      "   macro avg       0.69      0.58      0.41        20\n",
      "weighted avg       0.79      0.45      0.37        20\n",
      "\n",
      "------------------------------\n",
      "Model: Gemma2-9b-it\n",
      "Accuracy: 0.4500\n",
      "Precision: 1.0000\n",
      "Recall: 0.1538\n",
      "F1 Score: 0.2667\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.39      1.00      0.56         7\n",
      "    Positive       1.00      0.15      0.27        13\n",
      "\n",
      "    accuracy                           0.45        20\n",
      "   macro avg       0.69      0.58      0.41        20\n",
      "weighted avg       0.79      0.45      0.37        20\n",
      "\n",
      "------------------------------\n",
      "Model: deepseek-r1-distill-llama-70b\n",
      "Accuracy: 0.4500\n",
      "Precision: 1.0000\n",
      "Recall: 0.1538\n",
      "F1 Score: 0.2667\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.39      1.00      0.56         7\n",
      "    Positive       1.00      0.15      0.27        13\n",
      "\n",
      "    accuracy                           0.45        20\n",
      "   macro avg       0.69      0.58      0.41        20\n",
      "weighted avg       0.79      0.45      0.37        20\n",
      "\n",
      "------------------------------\n",
      "Model: qwen-qwq-32b\n",
      "Accuracy: 0.4500\n",
      "Precision: 1.0000\n",
      "Recall: 0.1538\n",
      "F1 Score: 0.2667\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.39      1.00      0.56         7\n",
      "    Positive       1.00      0.15      0.27        13\n",
      "\n",
      "    accuracy                           0.45        20\n",
      "   macro avg       0.69      0.58      0.41        20\n",
      "weighted avg       0.79      0.45      0.37        20\n",
      "\n",
      "------------------------------\n",
      "==================================================\n",
      "\n",
      "Results for prompting strategy: zero_shot_cot\n",
      "Model: llama-3.1-8b-instant\n",
      "Accuracy: 0.4500\n",
      "Precision: 1.0000\n",
      "Recall: 0.1538\n",
      "F1 Score: 0.2667\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.39      1.00      0.56         7\n",
      "    Positive       1.00      0.15      0.27        13\n",
      "\n",
      "    accuracy                           0.45        20\n",
      "   macro avg       0.69      0.58      0.41        20\n",
      "weighted avg       0.79      0.45      0.37        20\n",
      "\n",
      "------------------------------\n",
      "Model: Gemma2-9b-it\n",
      "Accuracy: 0.4500\n",
      "Precision: 1.0000\n",
      "Recall: 0.1538\n",
      "F1 Score: 0.2667\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.39      1.00      0.56         7\n",
      "    Positive       1.00      0.15      0.27        13\n",
      "\n",
      "    accuracy                           0.45        20\n",
      "   macro avg       0.69      0.58      0.41        20\n",
      "weighted avg       0.79      0.45      0.37        20\n",
      "\n",
      "------------------------------\n",
      "Model: deepseek-r1-distill-llama-70b\n",
      "Accuracy: 0.4500\n",
      "Precision: 1.0000\n",
      "Recall: 0.1538\n",
      "F1 Score: 0.2667\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.39      1.00      0.56         7\n",
      "    Positive       1.00      0.15      0.27        13\n",
      "\n",
      "    accuracy                           0.45        20\n",
      "   macro avg       0.69      0.58      0.41        20\n",
      "weighted avg       0.79      0.45      0.37        20\n",
      "\n",
      "------------------------------\n",
      "Model: qwen-qwq-32b\n",
      "Accuracy: 0.4500\n",
      "Precision: 1.0000\n",
      "Recall: 0.1538\n",
      "F1 Score: 0.2667\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.39      1.00      0.56         7\n",
      "    Positive       1.00      0.15      0.27        13\n",
      "\n",
      "    accuracy                           0.45        20\n",
      "   macro avg       0.69      0.58      0.41        20\n",
      "weighted avg       0.79      0.45      0.37        20\n",
      "\n",
      "------------------------------\n",
      "==================================================\n",
      "\n",
      "Results for prompting strategy: few_shot_cot\n",
      "Model: llama-3.1-8b-instant\n",
      "Accuracy: 0.4500\n",
      "Precision: 1.0000\n",
      "Recall: 0.1538\n",
      "F1 Score: 0.2667\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.39      1.00      0.56         7\n",
      "    Positive       1.00      0.15      0.27        13\n",
      "\n",
      "    accuracy                           0.45        20\n",
      "   macro avg       0.69      0.58      0.41        20\n",
      "weighted avg       0.79      0.45      0.37        20\n",
      "\n",
      "------------------------------\n",
      "Model: Gemma2-9b-it\n",
      "Accuracy: 0.4500\n",
      "Precision: 1.0000\n",
      "Recall: 0.1538\n",
      "F1 Score: 0.2667\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.39      1.00      0.56         7\n",
      "    Positive       1.00      0.15      0.27        13\n",
      "\n",
      "    accuracy                           0.45        20\n",
      "   macro avg       0.69      0.58      0.41        20\n",
      "weighted avg       0.79      0.45      0.37        20\n",
      "\n",
      "------------------------------\n",
      "Model: deepseek-r1-distill-llama-70b\n",
      "Accuracy: 0.4500\n",
      "Precision: 1.0000\n",
      "Recall: 0.1538\n",
      "F1 Score: 0.2667\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.39      1.00      0.56         7\n",
      "    Positive       1.00      0.15      0.27        13\n",
      "\n",
      "    accuracy                           0.45        20\n",
      "   macro avg       0.69      0.58      0.41        20\n",
      "weighted avg       0.79      0.45      0.37        20\n",
      "\n",
      "------------------------------\n",
      "Model: qwen-qwq-32b\n",
      "Accuracy: 0.4500\n",
      "Precision: 1.0000\n",
      "Recall: 0.1538\n",
      "F1 Score: 0.2667\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.39      1.00      0.56         7\n",
      "    Positive       1.00      0.15      0.27        13\n",
      "\n",
      "    accuracy                           0.45        20\n",
      "   macro avg       0.69      0.58      0.41        20\n",
      "weighted avg       0.79      0.45      0.37        20\n",
      "\n",
      "------------------------------\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# Compute and display classification metrics for each prompting strategy and each model\n",
    "for strategy in prompting_strategies:\n",
    "    print(f\"\\nResults for prompting strategy: {strategy}\")\n",
    "    for model in models:\n",
    "        true_labels = results[strategy][model]['true_labels']\n",
    "        preds = results[strategy][model]['predictions']\n",
    "        \n",
    "        true_binary = [1 if label == \"Positive\" else 0 for label in true_labels]\n",
    "        pred_binary = [1 if label == \"Positive\" else 0 for label in preds]\n",
    "        \n",
    "        accuracy = accuracy_score(true_binary, pred_binary)\n",
    "        precision = precision_score(true_binary, pred_binary, zero_division=0)\n",
    "        recall = recall_score(true_binary, pred_binary, zero_division=0)\n",
    "        f1 = f1_score(true_binary, pred_binary, zero_division=0)\n",
    "        \n",
    "        print(f\"Model: {model}\")\n",
    "        print(f\"Accuracy: {accuracy:.4f}\")\n",
    "        print(f\"Precision: {precision:.4f}\")\n",
    "        print(f\"Recall: {recall:.4f}\")\n",
    "        print(f\"F1 Score: {f1:.4f}\")\n",
    "        print(\"Classification Report:\")\n",
    "        print(classification_report(true_binary, pred_binary, target_names=[\"Negative\", \"Positive\"]))\n",
    "        print(\"------------------------------\")\n",
    "    print(\"==================================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b54d43a",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook demonstrated the implementation of multiple prompting techniques for SST-2 sentiment classification using Groq-hosted LLMs.\n",
    "\n",
    "For each of the five prompting strategies (Zero-Shot, Task Explanation, In-Context, Zero-Shot Chain-of-Thought, and Few-Shot Chain-of-Thought), the notebook collects predictions on 20 validation samples and computes standard classification metrics.\n",
    "\n",
    "Replace the simulation functions with actual API calls and adjust parameters as necessary for your production setup."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
