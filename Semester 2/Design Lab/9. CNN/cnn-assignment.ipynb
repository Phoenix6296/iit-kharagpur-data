{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torchvision import datasets, transforms, models\nfrom torch.utils.data import DataLoader, random_split\nimport os\nimport copy\nimport time\nimport numpy as np\nimport random\n\n# Set the random seed for reproducibility\ndef set_seed(seed=71):\n    torch.manual_seed(seed)\n    np.random.seed(seed)\n    random.seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n\nset_seed()","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-03-23T11:31:23.105795Z","iopub.execute_input":"2025-03-23T11:31:23.106184Z","iopub.status.idle":"2025-03-23T11:31:23.113339Z","shell.execute_reply.started":"2025-03-23T11:31:23.106156Z","shell.execute_reply":"2025-03-23T11:31:23.112534Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"# For training: resize, then random crop, and apply horizontal flip and normalization.\ntrain_transforms = transforms.Compose([\n    transforms.Resize(256),                    # Resize to 256 on the shorter side\n    transforms.RandomCrop(224),                # Random crop to 224x224\n    transforms.RandomHorizontalFlip(),\n    transforms.ToTensor(),\n    # Use ImageNet normalization if using a pretrained AlexNet\n    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                         std=[0.229, 0.224, 0.225])\n])\n\n# For validation: resize, then center crop, and apply normalization.\ntest_transforms = transforms.Compose([\n    transforms.Resize(256),\n    transforms.CenterCrop(224),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                         std=[0.229, 0.224, 0.225])\n])\n\n# Load CIFAR10 dataset using torchvision.datasets\ndataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=train_transforms)\n\n# Split the dataset into 80% training and 20% validation using a fixed seed\ntrain_size = int(0.8 * len(dataset))\nval_size = len(dataset) - train_size\ntrain_dataset, val_dataset = random_split(dataset, [train_size, val_size],\n                                            generator=torch.Generator().manual_seed(10))\n# Update validation dataset to use test transforms\nval_dataset.dataset.transform = test_transforms\n\n# Create DataLoaders for training and validation\ntrain_loader = DataLoader(train_dataset, batch_size=128, shuffle=True, num_workers=2)\nval_loader = DataLoader(val_dataset, batch_size=128, shuffle=False, num_workers=2)\n\ndataloaders = {'train': train_loader, 'val': val_loader}\ndataset_sizes = {'train': len(train_dataset), 'val': len(val_dataset)}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-23T11:31:30.775135Z","iopub.execute_input":"2025-03-23T11:31:30.775428Z","iopub.status.idle":"2025-03-23T11:31:31.701759Z","shell.execute_reply.started":"2025-03-23T11:31:30.775407Z","shell.execute_reply":"2025-03-23T11:31:31.701054Z"}},"outputs":[{"name":"stdout","text":"Files already downloaded and verified\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n\n# Load AlexNet model; use pretrained weights (or set pretrained=False to train from scratch)\nmodel = models.alexnet(pretrained=True)\n# Modify the final fully connected layer to output 10 classes (for CIFAR10)\nnum_ftrs = model.classifier[6].in_features\nmodel.classifier[6] = nn.Linear(num_ftrs, 10)\nmodel = model.to(device)\n\n# Define loss criterion and optimizer (using Adam)\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n\n# Learning rate scheduler: Reduce learning rate on plateau\nscheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=3, verbose=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-23T11:31:38.874451Z","iopub.execute_input":"2025-03-23T11:31:38.874938Z","iopub.status.idle":"2025-03-23T11:31:39.678576Z","shell.execute_reply.started":"2025-03-23T11:31:38.874907Z","shell.execute_reply":"2025-03-23T11:31:39.677964Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"# Early stopping parameters\npatience = 5  # epochs to wait without improvement\nbest_loss = float('inf')\nbest_model_wts = copy.deepcopy(model.state_dict())\nepochs_no_improve = 0\nnum_epochs = 30  # maximum number of epochs\n\nsince = time.time()\nprint(\"Starting training ...\")\n\nfor epoch in range(num_epochs):\n    print(f'Epoch {epoch+1}/{num_epochs}')\n    print('-' * 10)\n    \n    # Each epoch has a training and validation phase\n    for phase in ['train', 'val']:\n        if phase == 'train':\n            model.train()  # set model to training mode\n        else:\n            model.eval()   # set model to evaluation mode\n        \n        running_loss = 0.0\n        running_corrects = 0\n        \n        # Iterate over data.\n        for inputs, labels in dataloaders[phase]:\n            inputs = inputs.to(device)\n            labels = labels.to(device)\n            \n            optimizer.zero_grad()\n            \n            # Forward pass; track gradients only in train phase\n            with torch.set_grad_enabled(phase == 'train'):\n                outputs = model(inputs)\n                loss = criterion(outputs, labels)\n                _, preds = torch.max(outputs, 1)\n                \n                if phase == 'train':\n                    loss.backward()\n                    optimizer.step()\n            \n            running_loss += loss.item() * inputs.size(0)\n            running_corrects += torch.sum(preds == labels.data)\n        \n        epoch_loss = running_loss / dataset_sizes[phase]\n        epoch_acc = running_corrects.double() / dataset_sizes[phase]\n        \n        print(f'{phase.capitalize()} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n        \n        # Validate and update scheduler\n        if phase == 'val':\n            scheduler.step(epoch_loss)\n            if epoch_loss < best_loss:\n                best_loss = epoch_loss\n                best_model_wts = copy.deepcopy(model.state_dict())\n                epochs_no_improve = 0\n                print(\"Validation loss decreased, saving model ...\")\n            else:\n                epochs_no_improve += 1\n    \n    # Early stopping check\n    if epochs_no_improve == patience:\n        print(\"Early stopping triggered.\")\n        break\n    \n    print()\n\ntime_elapsed = time.time() - since\nprint(f'Training complete in {time_elapsed//60:.0f}m {time_elapsed % 60:.0f}s')\nprint(f'Best Validation Loss: {best_loss:.4f}')\n\n# Load best model weights\nmodel.load_state_dict(best_model_wts)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-23T11:31:45.610509Z","iopub.execute_input":"2025-03-23T11:31:45.610852Z","iopub.status.idle":"2025-03-23T11:51:25.377864Z","shell.execute_reply.started":"2025-03-23T11:31:45.610792Z","shell.execute_reply":"2025-03-23T11:51:25.376902Z"}},"outputs":[{"name":"stdout","text":"Starting training ...\nEpoch 1/30\n----------\nTrain Loss: 1.5629 Acc: 0.4244\nVal Loss: 1.2736 Acc: 0.5500\nValidation loss decreased, saving model ...\n\nEpoch 2/30\n----------\nTrain Loss: 1.0190 Acc: 0.6454\nVal Loss: 0.9539 Acc: 0.6636\nValidation loss decreased, saving model ...\n\nEpoch 3/30\n----------\nTrain Loss: 0.8283 Acc: 0.7144\nVal Loss: 0.7318 Acc: 0.7500\nValidation loss decreased, saving model ...\n\nEpoch 4/30\n----------\nTrain Loss: 0.7154 Acc: 0.7551\nVal Loss: 0.7511 Acc: 0.7468\n\nEpoch 5/30\n----------\nTrain Loss: 0.6316 Acc: 0.7849\nVal Loss: 0.7066 Acc: 0.7606\nValidation loss decreased, saving model ...\n\nEpoch 6/30\n----------\nTrain Loss: 0.5763 Acc: 0.8031\nVal Loss: 0.6459 Acc: 0.7859\nValidation loss decreased, saving model ...\n\nEpoch 7/30\n----------\nTrain Loss: 0.5397 Acc: 0.8164\nVal Loss: 0.6511 Acc: 0.7832\n\nEpoch 8/30\n----------\nTrain Loss: 0.4905 Acc: 0.8319\nVal Loss: 0.6236 Acc: 0.7954\nValidation loss decreased, saving model ...\n\nEpoch 9/30\n----------\nTrain Loss: 0.4737 Acc: 0.8384\nVal Loss: 0.7212 Acc: 0.7679\n\nEpoch 10/30\n----------\nTrain Loss: 0.4511 Acc: 0.8484\nVal Loss: 0.6177 Acc: 0.8053\nValidation loss decreased, saving model ...\n\nEpoch 11/30\n----------\nTrain Loss: 0.4138 Acc: 0.8610\nVal Loss: 0.6357 Acc: 0.7937\n\nEpoch 12/30\n----------\nTrain Loss: 0.4081 Acc: 0.8649\nVal Loss: 0.6643 Acc: 0.7959\n\nEpoch 13/30\n----------\nTrain Loss: 0.3818 Acc: 0.8711\nVal Loss: 0.6128 Acc: 0.8059\nValidation loss decreased, saving model ...\n\nEpoch 14/30\n----------\nTrain Loss: 0.3692 Acc: 0.8765\nVal Loss: 0.6091 Acc: 0.8097\nValidation loss decreased, saving model ...\n\nEpoch 15/30\n----------\nTrain Loss: 0.3405 Acc: 0.8871\nVal Loss: 0.5951 Acc: 0.8099\nValidation loss decreased, saving model ...\n\nEpoch 16/30\n----------\nTrain Loss: 0.3225 Acc: 0.8920\nVal Loss: 0.6117 Acc: 0.8160\n\nEpoch 17/30\n----------\nTrain Loss: 0.3202 Acc: 0.8963\nVal Loss: 0.6426 Acc: 0.8095\n\nEpoch 18/30\n----------\nTrain Loss: 0.3159 Acc: 0.8969\nVal Loss: 0.6813 Acc: 0.7990\n\nEpoch 19/30\n----------\nTrain Loss: 0.3105 Acc: 0.8988\nVal Loss: 0.7420 Acc: 0.7985\n\nEpoch 20/30\n----------\nTrain Loss: 0.1387 Acc: 0.9537\nVal Loss: 0.6209 Acc: 0.8398\nEarly stopping triggered.\nTraining complete in 19m 40s\nBest Validation Loss: 0.5951\n","output_type":"stream"},{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"<All keys matched successfully>"},"metadata":{}}],"execution_count":10},{"cell_type":"code","source":"# Save the best model's weights\ntorch.save(model.state_dict(), 'alexnet.pth')\nprint(\"Model saved as alexnet.pth\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-23T11:55:00.767094Z","iopub.execute_input":"2025-03-23T11:55:00.767439Z","iopub.status.idle":"2025-03-23T11:55:01.235706Z","shell.execute_reply.started":"2025-03-23T11:55:00.767415Z","shell.execute_reply":"2025-03-23T11:55:01.234917Z"}},"outputs":[{"name":"stdout","text":"Model saved as alexnet.pth\n","output_type":"stream"}],"execution_count":11}]}